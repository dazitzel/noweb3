% this file generated by notangle -- do not edit
\documentclass{article}
\usepackage{nchicago,multicol,array,tabularx,afterpage}
\usepackage[hypertex]{hyperref}

\setlength{\extrarowheight}{1pt}

\title{The {\tt noweb} Hacker's Guide}
\author{Norman Ramsey\thanks{Author's current address is Department of
  Electrical Engineering and Computer Science,
  Harvard University,
  33 Oxford Street,
  Cambridge, MA 02138, USA;
send email to {\tt nr@eecs.harvard.edu}.}\\Department of Computer Science\\
Princeton University}
\date{September 1992\\(Revised August 1994, December 1997, June 1998,
May 1999, October 2002)}

\setcounter{secnumdepth}{0}
\setcounter{tocdepth}{3}
\clubpenalty=10000
\widowpenalty=10000

\newcommand\kw[1]{\texttt{@#1}}
\newcommand\kws[2]{\kw{#1}\hbox{\thinspace}\ldots~\kw{#2}}
\newcommand\ikw[1]{\kw{index~#1}}
\newcommand\ikws[2]{\ikw{#1}\hbox{\thinspace}\ldots~\ikw{#2}}
\newcommand\xkw[1]{\kw{xref~#1}}
\newcommand\xkws[2]{\xkw{#1}\hbox{\thinspace}\ldots~\xkw{#2}}

\newcommand\nkw[1]{\texttt{NW\_#1}}
\newcommand\nkws[2]{\nkw{#1}\hbox{\thinspace}\ldots~\nkw{#2}}
\newcommand\nikw[1]{\nkw{INDEX\_#1}}
\newcommand\nikws[2]{\nikw{#1}\hbox{\thinspace}\ldots~\nikw{#2}}
\newcommand\nxkw[1]{\nkw{XREF\_#1}}
\newcommand\nxkws[2]{\nxkw{#1}\hbox{\thinspace}\ldots~\nxkw{#2}}

\newcommand\opencite[1]{{\let\hbox\relax\cite{#1}}}

\newenvironment{luafuns}{\tabularx{\textwidth}{@{\hskip1.5em}>{\ttfamily}lX@{}}}
                        {\endtabularx}

% l2h argblock kw <tt>@ </tt>
% l2h argblock kws <tt>@ ...#</tt>@
% l2h argblock ikw <tt>@index# </tt>
% l2h argblock ikws <tt>@index# ...#</tt>@index#
% l2h argblock xkw <tt>@xref# </tt>
% l2h argblock xkws <tt>@xref# ...#</tt>@xref#

\newcommand\itlabel{{\rm\it label\/}}
\newcommand\tag{{\rm\it tag\/}}
\newcommand\ident{{\rm\it ident\/}}

% l2h substitution itlabel <i>label</i>
% l2h substitution tag <i>tag</i>
% l2h substitution ident <i>ident</i>


% title in a table

\newcommand\ttitle[1]{\noalign{\medskip}\multicolumn{2}{c}{#1}\\\noalign{\smallskip}}

% l2h argblock ttitle <br><center> </center><br>

% figure hacking

\newcommand\topfigrule{%
  \vbox to 0pt{
     \vskip 5pt
     \centerline{\vrule height 1pt depth 0pt width 3in}
     \vss}}

\newcommand\botfigrule{%
  \vbox to 0pt{
     \vss
     \centerline{\vrule height 1pt depth 0pt width 3in}
     \vskip 5pt}}

\begin{document}
\label{roly-poly-fish-heads}
\maketitle

\begin{abstract}
{\tt Noweb} is unique among literate-programming tools in its
pipelined architecture, which makes it easy for users to change
its behavior or to add new features, without even recompiling.
This guide describes the representation used in the
pipeline and the behavior of the existing pipeline stages.
Ordinary users will find nothing of interest here; the guide is
addressed to those who want to change or extend {\tt noweb}.
\end{abstract}

\clearpage

\tableofcontents
\listoftables

\newpage

\section{Introduction}

This document is the Noweb Hacker's Guide.  It's job is to explain how
{\tt noweb} works internally, to make it possible for would-be hackers to
understand the current parts of {\tt noweb} and/or to add new features to it.

You should already be familiar with the concepts of Literate Programming,
as well as with the {\tt noweb} input format and how to use the {\tt no} program
from the command line.  To this end, reading
\citeN{knuth:litprog}
and the
\emph{no}(1),
\emph{noweave}(1),
and
\emph{notangle}(1)
man pages
will be helpful, as well as perusing some previously-written
{\tt noweb}
literate programs.

\section{The Big Picture}

The {\tt noweb} program parses the {\tt noweb} input format and turns it into a
stream of information.  At a high level, the stream is a sequence of
\emph{chunks} (both code and documentation).

Documentation chunks contain three things: text, newlines, and quoted
code.  Code chunks contain three things as well: text, newlines and
\emph{uses} of other chunks.

This format is ripe for analysis and transformation.  For example, it
is possible to insert cross-reference tags for references between chunks
(familiar to the reader of most any {\tt noweb} document).  Or it is possible
to insert information to support identifier cross-reference.  The entire
stream may be converted to a stream of LaTeX or a stream of HTML or ....

The stream format also makes it possible to play ``tricks,'' such as
counting identifier usages, noting identifier definitions, and so on.
Pretty-printing of code is possible simply by processing code chunks
appropriately.
% FIXME: WHAT OTHER TRICKS DID YOU HAVE IN MIND? I'M NOT FAMILIAR ENOUGH WITH THINGS

The purpose of the Hacker's Guide is to enable you to play your own
games with {\tt noweb}'s internals---and you never have to touch a line of
{\tt noweb}'s official code.

\section{{\tt noweb} Structure}

This section introduces the {\tt noweb} archicteture, and
describes the implemenation of {\tt noweb} 3.

\subsection{Pipeline Structure}

What makes {\tt noweb} interesting, flexible and unique among Literate
Programming tools is its \emph{pipelined} architecture.  By analogy with
Unix pipelines, each task is managed by a separate module.
This is illustrated in the following figures:

\begin{figure}[t]
\noindent
\begin{tabbing}
XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\={}\kill
\>\+{\tt markup}: Convert to pipeline representation\+\\
{\tt nt:} Extract desired chunk to standard output
\end{tabbing}
\caption{Stages in pipeline for {\tt notangle}}
\label{fig:pipe-notangle}

\noindent
\begin{tabbing}
XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\={}\kill
\>\+{\tt markup}: Convert to pipeline representation\+\\
{\tt autodefs.c}: Find definitions in C code\+\\
{\tt finduses -noquote}: Find uses of defined identifiers\+\\
{\tt noidx}: Add index and cross-reference information\+\\
{\tt totex}: Convert to {\LaTeX}
\end{tabbing}
\caption{Stages in pipeline for {\tt noweave -index -autodefs c}}
\label{fig:pipe-noweave}
\end{figure}

\subsection{Original Implementation}

For {\tt noweb} versions 1 and 2, the pipeline was actually implemented as a
Unix shell pipeline, using separate commands.  The {\tt markup} program parsed
the input file, generating the {\tt noweb} ASCII intermediate form
(described in the section
% FIXME: use a latex xref here
\emph{The ASCII pipeline representation}, below).
All subsequent stages of the pipeline (called
\emph{filters}, by analogy with Unix pipelines) read, processed, and
wrote the intermediate form.  The final stage generated the output \TeX{},
\LaTeX{}, HTML or source code.

This structure had a number of advantages:

\begin{itemize}
\item
\emph{Learnability.}
This architecture is easy for any programmer familiar with
Unix to understand immediately.
\item
\emph{Comprehendability.}
The parts of the {\tt noweb} system are small; each stage is easy to write.
This is also due to the simple intermediate format, which is highly
regular and easy for simple scripts written using Unix tools such as
{\tt sed} or {\tt awk} (or Icon, {\tt perl}, Python ...) to manipulate.
\item
\emph{Extensability.}
What you can do is limited largely by your imagination and your skill
as a programmer; just write filters as needed.
\item
\emph{``Portability.''}
Unix and its toolset are well known.  It is possible to write scripts
and/or C programs that compile and run across a wide variety of
commercial and free Unix systems.
\end{itemize}

While all of this sounds wonderful, in practice, this structure turned
out to have some significant disadvantages as well:

\begin{itemize}
\item
\emph{Performance.}
The use of a shell pipeline and ASCII representation are slow;
the data must be reparsed by every filter in the pipeline.
This is particularly expensive if the filter leaves most of the
data unmodified.
The data is copied as it passes through each stage in the
pipeline, increasing the total number of characters that the
system has to move before the final output is produced.
Also, having multiple processes ends up being
slower than doing everything in a single process.
\item
{\tt awk} \emph{limitations.}
Many of the tools distributed with {\tt noweb} were originally
written in {\tt awk}.  Unfortunately, {\tt awk} versions vary in language details,
and stock Unix versions of {\tt awk} often perform poorly.
\item
\emph{Non-standard tools.}
To remedy the problems with {\tt awk}, a number of filters were
rewritten using Icon.
Icon is great for writing filters but requires the {\tt noweb}
user to install a non-standard program; {\tt noweb} is no longer
a stand-alone piece of software.
In addition, Icon is not as popular as {\tt perl} or Python or
some of the other currently in-vogue scripting languages,
leading to a paucity of new filters, since the potential
filter author has to learn Icon first.
\item
\emph{System differences.}
The Unix shell and utilities vary across platforms more than
any sane person would like to deal with.  Although this situation
is improving, it is difficult for one person to be aware
of all the differences, and to write 100\% portable, correct
code.  (Although tools such as GNU Autoconf can help with this,
they are complicated in their own right, and seemed to be overkill
for a small tool such as {\tt noweb}.)
\end{itemize}

\subsection{Goals for the Rewrite}

Because of all the disadvantages listed previously, I decided
to rewrite {\tt noweb} to use a different structure.  The goals of
the rewrite were:

\begin{itemize}
\item
\emph{Maintain the pipeline concept, not implementation.}
It should remain easy to add filters to manipulate the
structure and content of a document before producing the final formatted
document, or compilable source code.
Ideally, adding filters should not require altering the
{\tt noweb} binary.
\item
\emph{Backwards compatibility with existing tools / filters.}
There exists a good-sized investment by myself and the user community
in tools that understand the textual version of the intermediate
format.  It should be possible (although at the risk of performance
loss) to continue to use these existing tools with the new
version of {\tt noweb}.
\item
\emph{Improve portability and ease of maintenance.}
The adopted design uses a minimum amount of C code, pushing
most of the work into pipeline stages written in a simple
embedded programming language.  All that has to be ported is
the mainline C code and the already highly-portable code for
the embedded language, which decreases the amount of work
and necessary knowledge about system variances.
Furthermore, the embedded language is simple enough that
I can maintain its interpreter with ``forever,'' with
very little effort.
\item
\emph{Improve performance by using internal pipelines}
The new version of {\tt noweb} passes all the data that represent
a document in memory.  It is only necessary to parse the
external format once, at the beginning.
\item
\emph{Make pipeline programming simple and learnable}
The internal representation is straightforward for programs to read,
modify, and write.  The embedded scripting language is easy to learn
and use.  The {\tt no} binary handles the mechanics of setting up internal
pipelines.  All that the author of a filter has to do is to actually
write the filter.
\end{itemize}


\section{Implementation, High level view}

The embedded language that {\tt noweb} uses is called `Lua 2.5+nw'
(or just Lua).
The primary pipeline stages, as well as the {\tt weave} and {\tt tangle}
driver programs, have been rewritten in Lua.
New filters should be written in Lua, and if anyone has the
time to help, existing filters should be translated into Lua.

Lua has the following advantages:

\begin{itemize}
\item
\emph{The language implementation is small and maintainable.}
Lua is a small language.  The embeddable interpreter is also
small and easy to maintain.  This was important, since I expect
to have to maintain the interpreter ``forever.''

\item
\emph{It is an easy language to program in.}
Lua is easy to learn.  The control-flow constructs are very standard,
as are the basic facilities for data management.

\item
\emph{Lua handles memory management automatically.}
Low-level memory management is the bane of the C and C++ programmer.
Having automatic garbage collection relieves the programmer's burden,
and ensures that memory usage doesn't explode in an unseemly manner.
It is particularly important for {\tt noweb}'s pipeline: data in the pipeline
isn't duplicated as it moves from stage to stage if it doesn't have to be.

\item
\emph{Supports modern concepts (OO, etc).}
Lua supports modern programming paradigms.  In particular, it is
easy to write Object-Oriented programs as the need arises.

\item
\emph{Extensable as necessary.}
The ``fallback'' mechanism for user-defined data types makes it
possible (and easy) to extend the language as necessary.  The
interpreter itself is also simple, making it possible to extend
the language if need be. (Although I have already modestly extended the
language, I don't foresee the need for additional language extensions.)

\item
\emph{Lua has Pattern matching and string manipulation.}
Because a {\tt noweb} document is largely text, it helps to be able to
work with regular expressions and do typical text manipulation
activities (substring, split, join, text match and substitution).
Lua supports all of these activities, so that the filter author
doesn't have to give up facilities she is used to from {\tt awk} or Icon.
\end{itemize}

The documentation for Lua 2.5+nw may be found in the directory
{\tt lua-2.5+nw/doc} in the {\tt noweb} distribution.  The file {\tt refman.ps} is the
reference manual, which describes the modified Lua language. (The primary
additions are the {\tt case} and {\tt globmatch} statements.)  You should now take
a detour, and read the language documentation for maximal understanding
of the rest of this guide, and before writing any filters.

The {\tt noweb} 3 software suite is now a single executable binary, named {\tt no}.
This program is written in C, and contains the embedded Lua interpreter.
The {\tt weave} and {\tt tangle} programs are now high level scripts written in
Lua that primarily parse command line arguments and set up the internal
pipeline.  See the \emph{noweave}\/(1) and \emph{notangle}\/(1) man pages for invocation
details.

Components of a {\tt noweb} pipeline are represented internally by what is
termed a \emph{stage}.
At the Lua level, a stage is a function that takes three arguments:
an input buffer, an output buffer, and an arbitrary third argument.
This latter is usually a list of options.

\begin{verbatim}
-- define my own stage named `mystage'
function Stages.mystage(inbuf, outbuf, args) 
do
        ...
end
\end{verbatim}

Buffers are described in detail in the next section.  For now, it's
enough to know that they contain the internal representation of the
original {\tt noweb} document.

There are three kinds of stages:

\begin{itemize}
\item
\emph{Sources}
A source ignores its input buffer and writes information into
its output buffer.  The {\tt markup} stage is the primary source; it
parses the {\tt noweb} input text file and generates the internal
representation used by the rest of the stages in the pipeline.
% [ Q. ARE THERE ANY OTHER SOURCES?  I DON'T THINK SO. ]

\item
\emph{Sinks}
A sink reads its input buffer and ignores its output buffer.
The {\tt totex}, {\tt tohtml} and {\tt nt} stages are the primary sinks.  They
generate \TeX{} or \LaTeX{}, HTML, and compilable source,
respectively.

\item
\emph{Filters}
A filter reads its input buffer, processes the data
in some fashion, and writes the transformed data to its output buffer.
\end{itemize}

Within the {\tt no} binary, Lua functions exist to compose sources,
sinks and filters into pipelines.  This is generally done by
the {\tt weave} and {\tt tangle} scripts for you; all you have to do is tell
{\tt weave} or {\tt tangle} which additional filter(s) to use for any
particular invocation, and they handle building the pipeline.

Internal filters are those written in Lua; they can read and write
the in-memory representation of the {\tt noweb} document directly.
External filters are those that work only on the ASCII intermediate
representation used by {\tt noweb} 2.  To leverage the existing base of
external filters, they may be included in what is termed a \emph{mixed}
pipeline, where some stages are internal and some are external.

The Lua filters {\tt Stages.toascii} and {\tt Stages.fromascii} convert the internal
representation to the external ASCII intermediate form and back.
Thus a mixed pipeline uses these stages to pass a document through an
external filter and then back into internal ones:

\begin{verbatim}
file -> markup -+                                        +-> totex -> .tex file
                +-> toascii -> extern_filt -> fromascii -+
\end{verbatim}

Again, this is handled transparently for you by {\tt weave} and {\tt tangle}.

To add your own filters invoke {\tt weave} or {\tt tangle} like so:

[ FIXME: EXAMPLES NEEDED HERE ]

\section{Implementation, Low level view}


In the Introduction, we said that {\tt noweb} files contain code and
documentation chunks, and described what each kind of chunk
could contain: text, newlines, quoted code, and references to
other chunks, as appropriate to the chunk type.

Each chunk is represented internally as a (key, value) pair.
The \emph{buffer} holds a sequence of these pairs, representing
the structure and contents of the original input file.  Each stage in the
{\tt noweb} pipeline manipulates this buffer, modifying it and passing
it on to the next stage.

\subsection{Keys}

The values of the predefined keys for the (key, value) pairs are predefined in Lua, with the
names given in Table~\ref{tab:lua-keys}.
These values are userdata values, with tag~\texttt{NWKEY}.
The Lua version accomadates non-predefined keys by using their string
values.

The \emph{structural keys}
represent the {\tt noweb} source syntax directly.
They must appear in particular orders that reflect the
structure of the source.

The \{emph{tagging keys} can be inserted
essentially anywhere (within reason), and with some exceptions, they are not generated
by {\tt markup}.

The \emph{wrapper keys}
mark the beginning and end of file,
and they carry information about what formatters are supposed to do in
the way of leading and trailing boilerplate.
They are used by {\tt no weave} but not by {\tt no tangle}, and they are
inserted directly by the {\tt no weave} Lua script, not by {\tt markup}.
FIXME: IS THIS LAST STATEMENT STILL CORRECT?

%FIXME: This table isn't formatting right, and it's beyond
%my abilities to fix.  Please review/edit.
\begin{table}[t]
\noindent
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
% l2h macro ttitle 1 </td></tr><tr><td colspan=2 align=center><b>#1</b></td></tr><td>
\ttitle{Structural keys}
\hline
NW\_BEGIN\_CODE $n$&Start a code chunk\\
NW\_END\_CODE $n$&End a code chunk\\
NW\_BEGIN\_DOCS  $n$&Start a documentation chunk\\
NW\_END\_DOCS  $n$&End a documentation chunk\\
NW\_TEXT {\rm\it string}&{\rm\it string} appeared in a chunk\\
NW\_NL&A newline appeared in a chunk\\
NW\_DEFN {\rm\it name}&The code chunk named {\rm\it name} is being defined\\
NW\_USE {\rm\it name}&A reference to code chunk named {\rm\it name}\\
NW\_QUOTE&Start of quoted code in a documentation chunk\\
NW\_ENDQUOTE&End of quoted code in a documentation chunk\\
\hline
\ttitle{Tagging keys}
\hline
NW\_FILE {\rm\it filename}&Name of the file from which the chunks came\\
NW\_LINE $n$&Next text line came from source line $n$ in current file\\
NW\_LANGUAGE {\rm\it language}&Programming language in which code is written\\
NW\_INDEX \ldots&Index information.\\
\hline
%FIXME: Norman, go through this part
\ttitle{Cross reference keys}
NW\_XREF\_LABEL&Cross reference to a label\\
NW\_XREF\_REF&Cross reference to a FIXME: reference??\\
NW\_XREF\_PREVDEF&Cross reference to the previous definition\\
NW\_XREF\_NEXTDEF&Cross reference to the next definition\\
NW\_XREF\_BEGINDEFS&Cross reference to the beginning of a definition\\
NW\_XREF\_DEFITEM&Cross reference to an item's definition\\
NW\_XREF\_ENDDEFS&Cross reference to the end of a definition\\
NW\_XREF\_BEGINUSES&Cross reference to the beginning of an item's use\\
NW\_XREF\_USEITEM&Cross reference to an item's use\\
NW\_XREF\_ENDUSES&Cross reference to the end of an item's use\\
NW\_XREF\_NOTUSED&Cross reference for an item that's not used\\
NW\_XREF\_BEGINCHUNKS&Cross reference to the start of several chunks FIXME: ??\\
NW\_XREF\_CHUNKBEGIN&Cross reference to the beginning of a chunk\\
NW\_XREF\_CHUNKUSE&Cross reference to a chunk's use\\
NW\_XREF\_CHUNKDEFN&Cross reference to a chunk's definition\\
NW\_XREF\_CHUNKEND&Cross reference to the end of a chunk\\
NW\_XREF\_ENDCHUNKS&Cross reference to the end of several chunks FIXME: ??\\
NW\_XREF\_TAG&Cross reference to a FIXME: tag??\\
\hline
\end{tabularx}\\
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Wrapper keys}
\hline
NW\_HEADER {\rm\it formatter options}&
      First line, identifying formatter and options\\
NW\_TRAILER {\rm\it formatter}&Last line, identifying formatter.\\
\hline
%\end{tabularx}\\
%\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Error key}
\hline
NW\_FATAL {\rm\it stagename} {\rm\it message}&
      A fatal error has occurred.\\
\hline
%\end{tabularx}\\
%\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Lying, cheating, stealing key}
\hline
NW\_LITERAL {\rm\it text}&
      Copy {\it text} to output.\\
\hline
\end{tabularx}

\caption{Keys used in {\tt noweb}'s Lua pipeline representation}
\label{tab:lua-keys}

\end{table}

% FIXME: From here forward is basically a clone of the original description
% for the noweb 2 representation, with edits made as appropriate.  Please
% review for accuracy.

\subsubsection{Structural keys}

The structural keys represent the chunks in the {\tt noweb} source.
Each chunk is bracketed by a
\nkws{BEGIN\_xxx}{END\_xxx} pair.
pair.  The use of \texttt{\_CODE} or \texttt{\_DOCS}
in the tag indicates what kind of chunk it is.
The \nkw{BEGIN} and \nkw{END} are numbered; within a single file,
numbers must be monotonically increasing, but they need not be
consecutive.
Filters may change chunk numbers at will.

Depending on its kind, a chunk may contain {\em documentation} or {\em
code}. 
Documentation may contain text and newlines, represented by \nkw{TEXT}
and \nkw{NL}.
It may also contain {\em quoted code} bracketed by
\nkws{QUOTE}{ENDQUOTE}.
Every \nkw{QUOTE} must be terminated by an \nkw{ENDQUOTE} within the
same chunk.
Quoted code corresponds to the \verb+[[+\ldots \verb+]]+ construct in
the {\tt noweb} source.

Code, whether it appears in quoted code or in a code chunk,
may contain text and newlines, and also definitions and uses of
code chunks, marked with \nkw{DEFN} and \nkw{USE}.
The first structural key in any code chunk must be \nkw{DEFN}.
\nkw{DEFN} may be preceded or followed by tagging keys, but the
next structural key
must be \nkw{NL}; 
together, the \nkw{DEFN} and \nkw{NL}
represent the initial \verb+<<chunk name>>=+
that starts the chunk (including the terminating newline).

A few facts follow from what's already stated above, but are probably
worth noting explicitly:
\begin{itemize}
\item 
Quoted code may not appear in code, nor may it appear in
\nkw{DEFN} or \nkw{USE}.
{\tt no weave} back ends are encouraged to give \verb+[[+\ldots
\verb+]]+ special treatment when it appears in \nkw{DEFN} or
\nkw{USE}, so that the text contained therein is treated as if it
were quoted code.
\item
The text in chunks may be distributed among as many \nkw{TEXT}
keys as desirable.  Any number of empty \nkw{TEXT} keys are
permitted.  In particular, it is not realistic to expect that a single
line will be represented in a single \nkw{TEXT} (see the discussion of
{\tt finduses} on page~\pageref{finduses}).
\item
{\tt markup} sometimes emit \nkw{USE} within
\nkws{QUOTE}{ENDQUOTE}, for example from a source like \verb+[[<<chunk name>>]]+.
\item
No two chunks have the same number.
\item
Because later filters can change chunk numbers, no filter should
plant references to chunk numbers anywhere in the pipeline.
\end{itemize}

\subsubsection{Tagging keys}

The structural keys carry all the code and documentation that
appears in a {\tt noweb} source file.
The tagging keys carry information about that code or
documentation.
The \nkw{FILE} key carries the name of the source file from which the
following lines come.
The \nkw{LINE} key gives the line number of the next \nkw{TEXT} line
within the current file (as determined by the most recent \nkw{FILE}
key). 
The only guarantee about where these appear is that {\tt markup}
introduces each new source file with a \nkw{FILE} that appears between
chunks.
Most filters ignore \nkw{FILE} and \nkw{LINE}, but {\tt tangle}
respects them, so that {\tt
no tangle} can properly mark line numbers if some {\tt noweb}
filter starts moving lines around.

\paragraph{Programming languages}

To support automatic indexing or prettyprinting, it's possible to
indicate the programming language in which a chunk is written.
The \nkw{LANGUAGE} key may appear at most once between
each \nkw{BEGIN\_CODE} and \nkw{END\_CODE} pair.
Standard values of \nkw{LANGUAGE} and their associated meanings are:
\begin{quote}
\begin{tabularx}{\linewidth}{@{}>{\ttfamily}lX@{}}
\texttt{awk}&awk\\
\texttt{c}&C\\
\texttt{c++}&C$++$\\
\texttt{caml}&CAML\\
\texttt{haskell}&Haskell\\
\texttt{html}&HTML\\
\texttt{icon}&Icon\\
\texttt{latex}&{\LaTeX} source\\
\texttt{lua}&Lua or Lua-2.5$+$nw\\
\texttt{lisp}&Lisp or Scheme\\
\texttt{make}&A Makefile\\
\texttt{m3}&Modula-3\\
\texttt{ocaml}&Objective CAML\\
\texttt{perl}&A perl script\\
\texttt{python}&Python\\
\texttt{sh}&A shell script\\
\texttt{sml}&Standard ML\\
\texttt{tex}&plain {\TeX}\\
\texttt{tcl}&tcl\\
\end{tabularx}
\end{quote}
If the \nkw{LANGUAGE} key catches on, it may be useful to create an
automatic registry on the World-Wide Web.

I have made it impossible to place \nkw{LANGUAGE} information directly
in a \texttt{noweb} source file.
My intent is that tools will identify the language of the root chunks
using any of several methods: conventional names of chunks, being told
on a command line, or identifying the language by looking at the
content of the chunks.
(Of these methods, the most practical is to name the root chunks after
the files to which they will be extracted, and to use the same naming
conventions as \texttt{make} to figure out what the contents are.)
A \texttt{noweb} filter will tag non-root chunks with the appropriate
\nkw{LANGUAGE} by propagating information from uses to definitions.
(This filter does not currently exist. Contributions gratefully accepted.)

\paragraph{Indexing and cross-reference concepts}


The index and cross-reference commands use \itlabel s, \ident s, and \tag s.
A \itlabel\ is a unique string generated to refer to some element of a
literate program.
They serve as labels or ``anchor points'' for back ends that are
capable of implementing their own cross-reference.
So, for example, the {\LaTeX} back end uses labels as arguments to \verb+\label+
and \verb+\ref+, and the HTML back end uses labels to name and refer
to anchors.
Labels never contain white space, which simplifies parsing.
The standard filters cross-reference at the chunk level, so that each
label refers to a particular code chunk, and all references to that
chunk use the same label.

An \ident\ refers to a source-language identifier.
{\tt noweb}'s concept of identifier is general; an identifier is
an arbitrary string.
It can even contain whitespace.
Identifiers are used as keys in the index; references to the same
string are assumed to denote the same identifier.

{\rm\it Tag\/}s are the strings used to identify components for
cross-reference in the final document.
For example, Classic {\tt WEB} uses consecutive ``section numbers'' to
refer to chunks.
{\tt noweb}, by default, uses ``sub-page references,'' e.g., ``24b''
for the second chunk appearing on page~24.
The HTML back end doesn't use any tags at all; instead, it
implements cross-referencing using the ``hot link'' mechanism.

The final step of cross-referencing involves generating tags and
associating a tag with each label.
All the existing back ends rely on a document formatter to do this
job, but that strategy might be worth changing.
Computing tags within a {\tt noweb} filter could be lots easier than
doing it in a formatter.
For example, a filter that computed sub-page numbers by grubbing in
{\tt .aux} files would be pretty easy to write, and it would eliminate
a lot of squirrelly {\LaTeX} code.

\paragraph{Index information}

I've divided the index keys into several groups.
There seems to be a plethora of keys, but most of them are
straightforward representations of parts of a document produced by
{\tt no weave}.  Readers may want to have a sample of {\tt no weave}'s
output handy when studying this and the next section.

\begin{table}
\begin{center}
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Definitions, uses, and {\tt @ \%def}}
\hline
NW\_INDEX\_DEFN \ident&The current chunk contains a definition of \ident\\
NW\_INDEX\_LOCALDEFN \ident&The current chunk contains a definition of
               \ident, which is not to be visible outside this file\\
NW\_INDEX\_use \ident&The current chunk contains a use of \ident\\
NW\_INDEX\_nl \ident&A newline that is part of markup, not part of the chunk\\
\hline
\ttitle{Identifiers defined in a chunk}
\hline
NW\_INDEX\_BEGINDEFS&Start list of identifiers defined in this chunk\\
NW\_INDEX\_ISUSED \itlabel&
        The identifier named in the following \nikw{DEFITEM} is used in
        the chunk labelled by \itlabel\\
NW\_INDEX\_DEFITEM \ident&
        \ident\ is defined in this chunk, and it is used in all the
        chunks named in the immediately preceding \nikw{ISUSED}.\\
NW\_INDEX\_ENDDEFS&End list of identifiers defined in this chunk\\
\hline
\ttitle{Identifiers used in a chunk}
\hline
NW\_INDEX\_BEGINUSES&Start list of identifiers used in this chunk\\
NW\_INDEX\_ISDEFINED \itlabel&
        The identifier named in the following \nikw{USEITEM} is defined in
        the chunk labelled by \itlabel\\
NW\_INDEX\_USEITEM \ident&
        \ident\ is used in this chunk, and it is defined in each of the
        chunks named in the immediately preceding \nikw{ISDEFINED}.\\
NW\_INDEX\_ENDUSES&End list of identifiers used in this chunk\\
\hline
\end{tabularx}\\
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{The index of identifiers}
\hline
NW\_INDEX\_BEGININDEX&Start of the index of identifiers\\
NW\_INDEX\_ENTRYBEGIN \itlabel\ \ident&
        Beginning of the entry for \ident, whose first definition is
        found at \itlabel\\
NW\_INDEX\_ENTRYUSE \itlabel&
        A use of the identifer named in the last \nikw{ENTRYBEGIN}
        occurs at the chunk labelled with \itlabel.\\
NW\_INDEX\_ENTRYDEFN \itlabel&
        A definition of the identifer named in the last \nikw{ENTRYBEGIN}
        occurs at the chunk labelled with \itlabel.\\
NW\_INDEX\_ENTRYEND&
        End of the entry started by the last \nikw{ENTRYBEGIN}\\
NW\_INDEX\_ENDINDEX&End of the index of identifiers\\
\hline
\end{tabularx}
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{FIXME: Keys Without Corresponding External Pipeline Explanation}
\hline
NW\_INDEX\_USE&FIXME: This is ...\\
NW\_INDEX\_NL&FIXME: This is ...\\
\hline
\end{tabularx}
\end{center}
\caption{Indexing keys}
\label{tab:lua-index}
\vskip -5pt
\end{table}

\paragraph{Definitions, uses, and {\tt @ \%def}}

The \nkw{INDEX} keys
\nikw{DEFN}, \nikw{USE}, and \nikw{NL} are the only ones
that appear in {\tt markup}'s output, and thus
which can appear in any program.
They may appear only within the boundaries of a code chunk
(\nkws{BEGIN\_CODE}{END\_CODE}).
\nikw{DEFN} and \nikw{USE} simply indicate that the current chunk
contains a definition or use of the identifier \ident\ which follows
the key.
The placement of \nikw{DEFN} need not bear a relationship to the
text of the definition, but \nikw{USE} is normally followed by a
\nkw{TEXT} that contains the source-code text identified as the
use.%
\footnote{This property can't hold when one identifier is a prefix of
another; see the description of {\tt finduses} on page~\pageref{finduses}.
FIXME: STILL TRUE?}

Instances of \nikw{DEFN} normally come from one of two sources: either a
language-dependent recognizer of definitions, or a hand-written
\verb+@ %def+ line.
In the latter case, the line is terminated by a newline that is
neither part of a code chunk nor part of a documentation chunk.
To keep line numbers accurate, that newline can't just be abandoned,
but neither can it be represented by \nkw{NL} in a documentation or
code chunk.
The solution is the \nikw{NL} key, which serves no purpose other
than to keep track of these newlines, so that back ends can produce
accurate line numbers.

Following a suggestion by Oren Ben-Kiki,
\nikw{LOCALDEFN} indicates a definition that is not to be visible
outside the current file.
It may be produced by a language-dependent recognizer or other filter.
Because I have questions about the need for \nikw{LOCALDEFN}, there is
officially no way to cause {\tt markup} to produce it.

\paragraph{Identifiers defined in a chunk}

The keys from \nikw{BEGINDEFS} to \nikw{ENDDEFS} are used to represent
a more complex data structure giving the list of identifiers defined
in a code chunk.
The constellation represents a list of identifiers; one \nikw{DEFITEM}
appears for each identifier.
The group also tells in what other chunks each identifier is used;
those chunks are listed by \nikw{ISUSED} keys which appear just
before \nikw{DEFITEM}.
The labels in these keys appear in the order of the corresponding
code chunks, and there are no duplicates.

These keys can appear anywhere inside a code chunk, but
filters are encouraged to keep these keys together.
The standard
filters guarantee that only
\nikw{ISUSED} and \nikw{DEFITEM} appear between \nikw{BEGINDEFS} and
\nikw{ENDDEFS}.
The standard filters put them at the end of the code chunk, which
simplifies translation by the {\LaTeX} back end, but that strategy
might change in the future.


It should go without saying, but the keys in these and all similar
groups (including some \nkw{XREF} groups) must be properly structured.
That is to say:
\begin{enumerate}
\item
Every \nikw{BEGINDEFS} must have a matching \nikw{ENDDEFS} within the
same code chunk.
\item
\nikw{ISUSED} and \nikw{DEFITEM} may appear only between matching
\nikw{BEGINDEFS} and \nikw{ENDDEFS}.
\item
The damn things can't be nested.
\end{enumerate}

\paragraph{Identifiers used in a chunk}


The keys from \nikw{BEGINUSES} to \nikw{ENDUSES} are the dual of
\nikw{BEGINDEF} to \nikw{ENDDEF};
the structure lists the identifiers used in the current code chunk,
with cross-references to the definitions.
Similar interpretations and restrictions apply.
Note that an identifier can be defined in more than one chunk,
although we expect that to be an unusual event.
{\hfuzz=1.2pt\par}

\paragraph{The index of identifiers}

Keys \nikw{BEGININDEX} to \nikw{ENDINDEX} represent the
complete index of all the identifiers used in the document.
Each entry in the index is bracketed by \nikws{ENTRYBEGIN}{ENTRYEND}.
An entry provides the name of the identifier, plus the labels of all the
chunks in which the identifier is defined or used.
The label of the first defining chunk
is given at the beginning of the entry so that back ends needn't
search for it.
{\hfuzz=4.9pt\par}

Filters are encouraged to keep these keys together.
The standard filters put them almost at the very end of the
{\tt noweb} file, just before the optional \nkw{TRAILER}.

\paragraph{Cross-reference information}

\newcommand\anchor{{\rmfamily\textit{anchor}}}

% l2h substitution anchor <i>anchor</i>

The most basic function of the cross-referencing keys is to
associate labels and pointers (cross-references) with elements of the
document, which is done with the \nxkw{REF} and \nxkw{LABEL} keys.
The other \nkw{XREF} keys all express chunk cross-reference
information that is emitted directly by one or more back ends.

Chunk cross-reference introduces the idea of
an {\anchor}, which is a label that refers to an ``interesting point''
we identify with the
beginning of a code chunk.
The anchor is the place we expect to turn to when we want to know about a
code chunk;
its exact value and interpretation depend on the back end being used.
The standard {\LaTeX} back end uses the sub-page number of the
defining chunk as the anchor, but the standard HTML back end uses some
\nkw{TEXT} from the documentation chunk preceding the code chunk.


\begin{table}
\begin{center}
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Basic cross-reference}
\hline
NW\_XREF\_LABEL \itlabel&Associates \itlabel\ with tagged item.\\
NW\_XREF\_REF \itlabel&
        Cross-reference from tagged item to item associated with \itlabel.\\
\hline
\ttitle{Linking previous and next definitions of a code chunk}
\hline
NW\_XREF\_PREVDEF \itlabel&
   The \nkw{DEFN} from the previous definition of this chunk is
   associated with \itlabel.\\
NW\_XREF\_NEXTDEF \itlabel&
   The \nkw{DEFN} from the next definition of this chunk is
   associated with \itlabel.\\
\hline
\ttitle{Continued definitions of the current chunk}
\hline
NW\_XREF\_BEGINDEFS&Start ``This definition is continued in \ldots''\\
NW\_XREF\_DEFITEM \itlabel&Gives the label of a chunk in which the
definition of the current chunk is continued.\\
NW\_XREF\_ENDDEFS&Ends the list of chunks where definition is continued.\\
\hline
\ttitle{Chunks where this code is used}
\hline
NW\_XREF\_BEGINUSES&Start ``This code is used in \ldots''\\
NW\_XREF\_USEITEM \itlabel&Gives the label of a chunk in which this
    chunk is used.\\
NW\_XREF\_ENDUSES&Ends the list of chunks in which this code is used.\\
NW\_XREF\_NOTUSED {\rm\it name}&
        Indicates that this chunk isn't used anywhere in this document.\\
\hline
\ttitle{The list of chunks}
\hline
NW\_XREF\_BEGINCHUNKS&Start of the list of chunks\\
NW\_XREF\_CHUNKBEGIN \itlabel\ {\it name}&
        Beginning of the entry for chunk {\it name}, whose {\anchor}
        is found at \itlabel.\\
NW\_XREF\_CHUNKUSE \itlabel&
        The chunk is used in the chunk labelled with \itlabel.\\
NW\_XREF\_CHUNKDEFN \itlabel&
        The chunk is defined in the chunk labelled with \itlabel.\\
NW\_XREF\_CHUNKEND&End of the entry started by the last \nxkw{CHUNKBEGIN}\\
NW\_XREF\_ENDCHUNKS&End of the list of chunks\\
\hline
\ttitle{Converting labels to tags}
\hline
NW\_XREF\_TAG \itlabel\ \tag&Associates \itlabel\ with \tag.\\
\hline
\end{tabularx}
\end{center}
\vskip -4pt
\caption{Cross-referencing keys}
\label{tab:lua-xref}
\vskip -3pt
\end{table}

\paragraph{Basic cross-reference}

\nxkw{LABEL} and \nxkw{REF} are named by analogy with the {\LaTeX}
\verb+\label+ and \verb+\ref+ commands.
\nxkw{LABEL} is used to associate a \itlabel\ with a succeeding item.
Items that can be so labelled include:
\begin{quote}
\begin{tabularx}{\linewidth}{>{\tt}l>{\raggedright\arraybackslash}X}
NW\_DEFN&Labels the code chunk that begins with this \rlap{\kw{defn}.}\\
                                                % cheating the line breaker
NW\_USE&Labels this particular use.\\
NW\_INDEX\_DEFN&Labels this definition of an identifier.\\
NW\_INDEX\_USE&Labels this use of an identifier.\\
NW\_TEXT&Typically labels part of a documentation chunk.\\
NW\_END\_DOCS&Typically labels an empty documentation chunk.\\
\end{tabularx}
\end{quote}
I haven't made up my mind whether this should be the complete set, but
these are the ones used by the standard filters.
Most back ends use the chunk as the basic unit of cross-reference, so
the labels of \nkw{DEFN} are the ones that are most often used.
The HTML back end, however, does something a little different---it
uses labels that refer to documentation preceding a chunk, because
typical HTML browsers place the label%
\footnote{The HTML terminology calls a label an ``anchor.''}
at the top of the screen, and using the label of the \nkw{DEFN} would
lose the documentation immediately preceding a chunk.
The labels used by this back end usually point to \nkw{TEXT}, but they
may point to \nkw{END\_DOCS} when no text is available.


\nxkw{REF} is used to associate a reference with a succeeding item.
Such items include
\begin{quote}
\begin{tabularx}{\linewidth}{l>{\raggedright\arraybackslash}X}
{\tt NW\_DEFN}, {\tt NW\_USE}&Refers to the label used as an {\anchor} for this chunk.\\
\vtop{\hbox{\strut{\tt NW\_INDEX\_DEFN},}\hbox{\strut{\tt NW\_INDEX\_USE}}}&
  Refers to the label used as an {\anchor} for the first
           chunk in which this identifier is defined.\\
\end{tabularx}
\end{quote}

\paragraph{Linking previous and next definitions of a code chunk}

\nxkw{PREVDEF} and \nxkw{NEXTDEF} may appear anywhere in a code chunk,
and they give the labels of the preceding and succeeding definitions
of that code chunk, if any.
Standard filters currently put them at the beginning of the code
chunk, following the initial \nkw{DEFN}, so the information can be used
with the \nkw{DEFN} key,
\`a la \citeN{fraser:retargetable:book}.

\paragraph{Continued definitions of the current chunk}

The keys ranging from \nxkw{BEGINDEFS} to \nxkw{ENDDEFS} appear in the first
definition of each code chunk.
They provide the information needed by the ``This definition is
continued in \ldots'' message printed by the standard {\LaTeX} back
end.
They can appear anywhere in a code chunk, but standard filters put
them after all the \nkw{TEXT} and \nkw{NL}s, so that back ends can just
print out text.


\paragraph{Chunks where this code is used}

The keys from \nxkw{BEGINUSES} to \nxkw{ENDUSES} are the dual of
\nxkw{BEGINDEFS} to \nxkw{ENDDEFS}; they show where the current chunk is
used.
As with \nxkws{BEGINDEFS}{ENDDEFS}, they appear only in the first
definition of any code chunk, and they come at the end.
Sometimes, as with root chunks, the code isn't used anywhere, in which
case \nxkw{NOTUSED} appears instead of \nxkws{BEGINUSES}{ENDUSES}.
The name of the current chunk appears as an argument to \nxkw{NOTUSED}
because some back ends may want to print a special message for unused
chunks---they might be written to files, for example.

\paragraph{The list of chunks}

The list of chunks, which is defined by the keys
\nxkws{BEGINCHUNKS}{ENDCHUNKS}, is the
analog of the index of identifiers, but it lists all the code chunks
in the document, not all the identifiers.

Filters are encouraged to keep these keys together.
The standard filters put them at the end of the {\tt noweb}
file, just before the index of identifiers.

\paragraph{Converting labels to tags}

None of the existing back ends actually computes tags; they all use
formatting engines to do the job.
The {\LaTeX} back end uses an elaborate macro package to compute
sub-page numbers, and the HTML back end arranges for ``hot links'' to
be used instead of textual tags.

Some people have argued that literate-programming tools shouldn't require
elaborate macro packages, that they should use the basic facilities
provided by a formatter.  Nuweb, for example, uses standard {\LaTeX}
commands only, but goes digging through {\tt .aux} files to find
labels and compute sub-page numbers.
Doing this kind of computation in a real programming language is much
easier than doing it with {\TeX} macros, and I expect that one day
{\tt noweb} will have a tag-computing filter, the results of
which will be expressed using the \nxkw{TAG} key.

The rules governing \nxkw{TAG} are that it can appear anywhere.
None of the standard filters or back ends does anything with it.



\subsubsection{Wrapper keys}

The wrapper keys, \nkw{HEADER} and \nkw{TRAILER}, are anomalous in
that they're not generated by {\tt markup} or by any of the
standard filters; instead they're inserted by the {\tt no weave} Lua
script at the very beginning and end of file.
FIXME: STILL TRUE?
The standard {\TeX}, {\LaTeX}, and HTML back ends use them to provide
preamble and postamble markup, i.e., boilerplate that usually has to
surround a document.
They're not required (sometimes you don't want that boilerplate), but
when they appear they must be the very first and last items in the
buffer, and the formatter names must match.

\subsubsection{Error key}

FIXME: Verify phrasing of this section w.r.t. Lua code.

The error key \nkw{FATAL} signifies that a fatal error has
occurred.
The pipeline stage originating such an error gives its own name and a
message, and it also writes a message to standard error.
Filters seeing \nkw{FATAL} must copy it to their output and terminate
themselves with error status.
Back ends seeing \nkw{FATAL} must terminate themselves with error
status. (They should not write anything to standard error since that
will have been done.)

Using \nkw{FATAL} enables shell scripts to detect
that something has gone wrong even if the only exit status they have
access to is the
exit status of the last stage in a pipeline.

\subsubsection{Lying, cheating, stealing key}

The \nkw{LITERAL} key is used to hack output directly into \texttt{no weave}
back ends, like \texttt{totex} and \texttt{tohtml}.
These back ends simply copy the text to their output.
Tangling back ends ignore \nkw{LITERAL}.
The \nkw{LITERAL} key is used by Master Hackers who are too lazy to
write new back ends.
Its use is deprecated.
It should not exist.

%=============================================================================

\subsection{Buffers}

As mentioned,
Lua buffers hold sequences of key-value pairs.
The following Lua functions are provided on buffers:\\
\begin{luafuns}
NWB.new&
Creates a new, empty buffer, which is userdata with tag
\texttt{NWBUFFER}.\\ 
NWB.reader&
Given one argument, a buffer, returns a ``reader.''
The reader is an abstraction capable of reading key-value
pairs; it initially points to the first pair in the buffer.
Readers are represented as userdata with tag \texttt{NWREADER}.\\
NWB.read&
Takes one argument, a reader, and returns \emph{two} results: the next
key and value in the reader.
If the pairs in the reader have been exhausted, both results are nil.\\
NWB.put&
Takes three arguments (buffer, key, value) and appends the key-value
pair to the end of the buffer.
The value may be omitted and defaults to nil.
It is not specified whether calling \texttt{NWB.put} to append to a
buffer changes the sequence returned by a previously created reader.
This means that if you write code that writes to and reads from the same
buffer at the same time, your code shouldn't depend on whether what
you write shows up when you read.
\\
NWB.pull&
Takes one argument, a buffer, and returns \emph{two} results: the final
key and value in the buffer.
Has the side effect of removing that pair from the buffer.
If the buffer is empty, both results are nil, and no side effects take
place.
The \texttt{pull} operation is the inverse of \texttt{put}.\\
\end{luafuns}

\subsection{Building pipelines}

Both the internal and external pipelines are available in Lua, but
this section discusses only the mixed pipeline, of which the other
two can be treated as special cases.

Pipelines are composed of \emph{stages}, which may be sources,
filters, or sinks.
The first stage in any pipeline must be a source, the last stage must
be a sink, and the remaining stages must be filters.
All are userdata, with the following tags:
\begin{luafuns}
MSOURCE&
A source, which can be connected to a sink to form a pipeline.\\
MSINK&
A sink, which can be connected to a source to form a pipeline.\\
MFILTER&
A filter, which can be connected to a source, sink, or filter without
changing its type.\\
MPIPE&
A pipeline, which can be run or printed.\\
\end{luafuns}
These things are created using the following functions:\\
\begin{luafuns}
MPipe.source&
Passed two arguments, to create source stages.
If the first argument is a string, it is taken to be the name of a
program to be run as an external pipeline stage, and 
the second argument should be a 
list of strings to be used as arguments.
If the first argument is a Lua function, it is run as an internal
pipeline stage, and the second argument is passed to it (along with a
buffer into which information should be emitted).\\
MPipe.xsource&
Like \texttt{MPipe.source}, except even Lua functions are run in
external pipeline stages.\\
MPipe.sink&
Like \texttt{MPipe.source}, except it creates a sink, not a source.\\
MPipe.filter&
Like \texttt{MPipe.source} and \texttt{MPipe.sink}, except it
creates a filter, not a source or sink.\\
MPipe.connect&
Takes two arguments, both stages, and connects them, returning a stage
or a pipeline, as described by the following table:
\begin{quote}
\begin{tabular}{|l|l||l|}
\hline
Left Stage& Right Stage& Result\\
\hline
Source&Filter&Source\\
Source&Sink&Pipeline\\
Filter&Filter&Filter\\
Filter&Sink&Sink\\
\hline
\end{tabular}
\end{quote}\\
MPipe.run&
Run a pipeline.\\
MPipe.print&
Print a textual representation of a pipeline or stage.\\
\end{luafuns}

\subsection{Writing Filters}

As Nicklaus Wirth once put it, ``Algorithms + Data = Programs.''
The data is the NWB buffer.
Your job is to supply the algorithm.
Writing a filter consists primarily
of writing a loop that reads key--value pairs from the input
buffer, uses a {\tt case} to find keys of interest, manipulates the
corresponding values, and then writes everything to the output
buffer.

As mentioned, filter functions take three arguments: an input buffer,
an output buffer and arbitrary third argument.  The third argument is
usually a list of command line options that can be used to modify
the behavior of the filter.

The typical structure of a filter function is:

\begin{verbatim}
function Stages.myfilter(inbuf, output, args)
do
    -- Typically, one processes the options first.  The following is
    -- essentially boiler-plate code that loops over the strings in
    -- the `args' list, looking for options and processing them.
    -- This would usually be done as a separate noweb chunk.

    -- Convert a string into a list
    if type(args) == "string" then args = List.fromstring(args) end

    -- Loop over the list
    while List.length(args) > 0 do
        local a = List.get(args) 
        case a of
        | "-foo" => ... -- code for foo
        | "-bar" => ... -- code for bar
        else => noweb_error("unknown argument ", a)
        end
    end

    -- Create a reader from the buffer
    local r = NWB.reader(inbuf)

    local key, val

    -- read initial key/value pair
    key, val = NWB.read(r)
    while key ~= nil do     -- loop until done
        case key of
        | .... => ....
        end
        NWB.put(outbuf, key, val)

        key, val = NWB.read(inbuf)
    end
end -- myfilter
\end{verbatim}

The following is (a slightly edited version of) the
{\tt Stages.elideuses} filter
from the {\tt noweb} distribution that illustrates this structure nicely:

\begin{verbatim}
function Stages.elideuse(sourcebuf, sinkbuf, globs)
  local r = NWB.reader(sourcebuf)
  if type(globs) == "string" then globs = List.fromstring(globs) end
  local eat = nil
  if globs[1] == "-eat" then List.get(globs); eat = List.get(globs) end
  local key, arg
  key, arg = NWB.read(r)
  local eliding = nil
  while key do 
    case key of
    | NW_USE =>
        <<set [[matches]] if [[arg]] matches a pattern in [[globs]]>>
        if matches then
          eliding = 1
        else
          NWB.put(sinkbuf, key, arg)
        end
    | NW_TEXT =>
        if eliding and strfind(arg, "^%s*$") then
          -- do nothing
        else
          eliding = nil
          NWB.put(sinkbuf, key, arg)
        end
    | NW_NL =>
        if eliding then
          local k, a = NWB.pull(sinkbuf)
          while k == NW_TEXT and strfind(a, "^%s*$") do
            k, a = NWB.pull(sinkbuf)
          end
          NWB.put(sinkbuf, k, a)
          if eat then NWB.put(sinkbuf, "literal", eat) end
          eliding = nil
        end
        NWB.put(sinkbuf, key, arg)
    else =>          
      NWB.put(sinkbuf, key, arg)
    end
    key, arg = NWB.read(r)
  end
  return 0
end
\end{verbatim}

\subsection{C version}

You'll get a pointer to the \texttt{nwbuffer} and \texttt{mpipe}
modules.

FIXME: NORMAN: THIS SECTION SHOULD EITHER BE REMOVED OR EXPANDED, BUT
I THINK YOU'LL HAVE TO DO IT.

\subsection{Miscellaneous Lua functions}

The {\tt no}  binary includes the following functions:\\
\begin{luafuns}
System.abort&
Abort the current process with a core dump.\\
System.getenv&
The C~\texttt{getenv} call---returns the value of an environment
variable.\\ 
System.putenv&
Adds a binding to the environment.
Takes one argument, a string of the form
\textit{name}\texttt{=}\textit{value}.\\
System.abort&
Calls  \texttt{abort()}, causing a core dump.\\
System.fflush&
Takes one argument, the file to be flushed---or with no argument,
flushes the current Lua output file.
Returns the return code from the C~\texttt{fflush} call.\\
alphacmp&
A comparison function for sorting.
Strings that differ only in case are treated as different, but they are
sorted adjacently.\\
calltrace&
If the argument is non-nil, traces every call and return of a Lua
function.
If the argument is nil, turns off tracing.\\
sort&
Takes a comparison function and a table, and sorts the table using the
C~library \emph{quicksort} function.\\
do\_nwfile&
Takes the name of a file, looks for it in the directories named by
\texttt{NWPATH}, and runs it through the Lua interpreter, returning the
status.\\
\end{luafuns}

\medskip
\noindent
The {\tt no} binary also loads the following functions written in Lua:\\
\begin{luafuns}
errormsg&
Takes up to 8~arguments and writes them to standard error.
Arguments may be strings, numbers, or lists.\\
noweb\_error&
Halts \texttt{noweb} with an error message consisting of up to
9~arguments.\\ 
pipetrace&
If given a non-null argument, starts tracing every call to
\texttt{NWB.read} and \texttt{NWB.put}.
Useful for debugging pipeline problems.
A null argument turns off tracing.\\
Unix.stringFilter&
Takes one argument, a Unix shell command, 
and returns a filter to execute that argument.\\
image&
Attempts to convert any Lua value to string form.\\
show&
Writes images of up to 6~arguments on the default Lua output.\\
funname&
Attempts to return a name by which a Lua value can be known.
Searches top-level variables, then top-level tables.
Typically used to find names of functions.\\
\end{luafuns}

\paragraph{Lists}
Noweb uses Lua tables to represent lists.
(A future version may use a specialized, more efficient data type.)
The first element in a list~\texttt{l} is \texttt{l[1]}, then
\texttt{l[2]}, etc.
The standard Lua literal notation may be used for lists.
The Lua concatenation operator ``\texttt{..}'' may be used to
concatenate lists.  It may also be used to concatenate strings and
lists, and fallbacks are set up to make \texttt{nil} an identity when
used with ``\texttt{..}'' and a list.
In addition to these operators,
the following functions are provided:\\
\begin{luafuns}
List.length&Number of elements in a list.\\
List.is&Returns non-nil if the argument is a list.\\
List.get&
Removes and returns the first element of a list.
Side-effects (modifies) the list.\\
List.put&
Takes two arguments, a list and an item, 
and appends the item to the end of the list.\\
List.flatten&
Takes the values in a list, converts them to strings with
\texttt{tostring}, and concatenates them.\\
List.concat&
Concatenates two lists, returning the new list as a result.\\
List.new&
Takes a table literal and returns a list, or 
given no argument, returns a fresh, empty list.
Recommended in case \texttt{noweb} moves to a more efficient
implementation of lists.\\
List.tostring&
Concatenates list elements as strings.
Takes a list and a second argument, the ``separator.''
Separator defaults to a space.\\
List.fromstring&
Takes a string, splits it on white space, and makes a list of the results.\\
\end{luafuns}

\paragraph{Sets}
Noweb uses Lua tables to represent sets.
(A future version may use a specialized, more efficient data type.)
The keys in the table are the set members; values are always~1.
The following functions are provided:\\
\begin{luafuns}
Set.length&
Number of elements in a set.\\
Set.members&
Returns list of the elements in a set.\\
Set.insert&
Takes a set and an item, and adds the item to the set.\\
Set.delete&
Takes a set and an item, and removes the item from the set, if present.\\
Set.member&
Takes a set and an item, and returns non-null if and only if the item
is in the set.\\
Set.diff&
\texttt{Set.diff(s1, s2)} returns a new set containing 
all of the elements of~\texttt{s1} that are not members of~\texttt{s2}.
Fallbacks are set up so the subtraction operator invokes \texttt{Set.diff}.\\
Set.new&
Takes a list of elements and returns a set containing those elements.
With no argument, returns a fresh, empty set.
Recommended in case \texttt{noweb} moves to a more efficient
implementation of sets.\\
\end{luafuns}



\section{Standard internal pipeline stages}

The standard internal stages and filters are stored in the tables
\texttt{Stages}, so it is always possible to get
a list by running \texttt{no} without arguments, and typing
\texttt{show(Stages)}.

Internal sources take two arguments: a buffer into which information
should be emitted, and an arbitrary argument used to pass
source-specific information.
In C~versions of internal stages, the arbitrary argument is a value of
type~\texttt{void~*}; in Lua versions, it may be a value of any type.
Internal sinks also take a buffer and an arbitrary argument, but
information is read from the buffer, not written.
An internal filter takes three arguments: an input buffer, an output
buffer, and an additional, arbitrary argument.

Please convert existing stages to internal form!

\subsection{Internal sources}
\subsubsection{The source \texttt{Stages.markup}}

\texttt{Stages.markup} is a source (or ``front end'') that converts an
ASCII \texttt{noweb} file into internal pipeline form.
Its arbitrary argument is a list of file names;
its output represents the contents of those files.
Each file is represented by a \nkw{FILE} and
{\it filename} key--value pair,
followed by a sequence of chunks.
{\tt Stages.markup} numbers chunks consecutively, starting at~0.
It also recognizes and undoes the escape sequence for double brackets,
e.g.~converting ``{\tt @{}<<}'' to ``{\tt <<}''.
The only tagging keywords found in its output are \nikw{DEFN} or
\nikw{NL}; despite what's written about it, \nikw{USE} never appears.
\subsubsection{The source \texttt{Stages.fromascii}}
 {\tt Stages.fromascii} is a source that reads 
an external ASCII pipeline from a file.
The arbitrary argument is the file from which the ASCII should be read; if omitted, 
it defaults to the current Lua input file.
Most people won't need to use this stage explicitly;
 the ``mixed pipeline'' abstraction inserts it automatically where needed.
\subsubsection{The filter \texttt{Stages.finduses}}

Using code contributed by Preston Briggs, this filter makes two passes
over its input.
The first pass reads in all the \nikw{DEFN} and \nikw{LOCALDEFN} pairs and builds an
Aho-Corasick recognizer%
\footnote{A.V.~Aho and M.~Corasick.
Efficient string matching: An aid to bibliographic search. Comm. ACM, 18(6):333--340, 1975.
This is the algorithm used for the Unix \texttt{fgrep} program.}
for the identifiers named therein.
The second pass copies the input, searching for these identifiers in
each \nkw{TEXT} pair that is in code.
When it finds an identifier, {\tt Stages.finduses} breaks the \nkw{TEXT} pair
into pieces, inserting \nikw{USE} immediately before the \nkw{TEXT}
piece that contains the identifier just found.%
\footnote{The behavior described would duplicate \nkw{TEXT} pieces
whenever one identifier was a prefix of another.
This event is rare, and probably undesirable, but it can happen if,
for example, the C$++$ names {\tt MyClass} and {\tt MyClass::Function}
are both considered identifiers.
In this case, whatever identifier is found first is emitted first, and
only the unemitted pieces of longer identifiers are emitted.}
{\tt Stages.finduses} assumes that previous filters will not have broken
\nkw{TEXT} value strings in the middle of identifiers.

The arbitrary argument is a list of strings, which includes a potential
option and a potential list of file names.
The \verb+-noquote+  option prevents {\tt Stages.finduses} from
searching for uses in quoted code.
If {\tt Stages.finduses} is given additional arguments, it takes those arguments to be
file names, and it reads lists of identifiers (one per line) from the
files so named, rather than from its input buffer.
This technique enables {\tt Stages.finduses} to make a single pass over its
input; {\tt no~weave} uses it to implement the {\tt -indexfrom} option.

{\tt Stages.finduses} shouldn't be run before filters which, like the {\tt
autodefs} filters, expect one line to be represented in a single
\nkw{TEXT} pair.
Filters (or back ends) that have to be run late, like 
prettyprinters, should be prepared to deal with lines broken into
pieces and with \nkw{INDEX} and \nkw{XREF} tags intercalated.
\subsection{Internal sinks}
\subsubsection{The sink \texttt{Stages.nt}}

The \texttt{Stages.nt} sink (back end) implements tangling.
By expanding all
uses to their definitions, 
it extracts the program defined by one or more chunks named in
arguments,
and it writes that program on standard
output.
Its arbitrary argument is a list of strings representing these options:
\begin{quote}
\begin{tabularx}{\linewidth}{lX}
\tt -t&Turn off expansion of tabs.\\
\tt -t$n$&Expand tabs on $n$-column boundaries.\\
\tt -R{\rmfamily\textit{name}}&Expand the code chunk named \textit{name}.\\
\tt -L{\rmfamily\textit{format}}&Use \textit{format} as the format string
        to emit line-number information.
\end{tabularx}
\end{quote}
See the man page for {\tt notangle} for details on the meanings of
these options and on the expansion performed by \texttt{Stages.nt}.
\subsubsection{The sink \texttt{Stages.toascii}}
 {\tt Stages.toascii} is a sink that converts the internal pipeline into
the external ASCII form.
The arbitrary argument is the file to which the ASCII should be written; if omitted, 
it defaults to the current Lua output file.
Most people won't need to use this stage explicitly;
 the ``mixed pipeline'' abstraction inserts it automatically where needed.
\subsubsection{The filter \texttt{Stages.cond}}

The filter \texttt{Stages.cond} enables the Noweb programmer to
include code chunks conditionally.
The filter is sensitive to chunk definitions in the following form:
\begin{quote}
\texttt{<<\textrm{\emph{name}} ((\textrm{\emph{condition}}))>>=}
\end{quote}
where \emph{name} is the name of the chunk, as usual, and
\emph{condition} is a restricted Boolean expression in Lua.
The \emph{condition} may not include function calls, and it may not
use Lua reserved words other than \texttt{and}, \texttt{or}, and~\texttt{not}.

The arbitrary argument is a list of identifiers; when the
\emph{condition} is evaluated, those identifiers are
taken to be non-nil, and all other identifiers are taken to be nil.
The chunk definition is included in
the output from \texttt{tangle} if and only 
if the \emph{condition} is true.
(The inclusion is implemented using Lee Wittenberg's trick of renaming
the definition to \texttt{<<\textrm{\emph{name}}>>}.)

\subsubsection{The filter \texttt{Stages.stripconds}}

The filter \texttt{Stages.stripconds} enables the Noweb programmer to
include code chunks conditionally.
The filter is sensitive to chunk definitions in the following form:
\begin{quote}
\texttt{<<\textrm{\emph{name}} ((\textrm{\emph{condition}}))>>=}
\end{quote}
where \emph{name} is the name of the chunk, as usual, and
\emph{condition} is a simple identifier.

The arbitrary argument is a list of identifiers; if
\emph{condition} is one of the identifiers, it is removed from the
definition, arranging that the chunk becomes a definition of
\texttt{<<\textrm{\emph{name}}>>}.
The effect is that
the chunk definition is included in
the output from \texttt{tangle} if and only 
if the \emph{condition} is one of the identifiers passed to 
\texttt{Stages.stripconds}.
\subsubsection{The sink \texttt{Stages.tohtml}}

This back end emits HTML.
It uses the formatter {\tt html} with \nkw{HEADER} and \nkw{TRAILER} to
emit suitable HTML boilerplate.
For other formatters (like {\tt none}) it emits no header or trailer.
Its arbitrary argument is a list of options:
\begin{quote}
\begin{tabularx}{\linewidth}{lX}
\tt -delay&Accepted, for compatibility with other back ends, but ignored.\\
\tt -localindex&Produces local identifier cross-reference after each code chunk.\\
\tt -raw&Wraps text generated for code chunks in a {\LaTeX} {\tt rawhtml}
environment, making the whole document suitable for processing with
{\tt latex2html}.\\
\end{tabularx}
\end{quote}
\subsubsection{The sink \texttt{Stages.totex}}

{\tt Stages.totex} implements both the plain {\TeX} and {\LaTeX} back ends,
using \nkw{HEADER} \texttt{tex} and \nkw{HEADER} \texttt{latex} to distinguish them.
When using a {\LaTeX} header, {\tt Stages.totex} places the optional text
following the header inside a \verb+\noweboptions+ command.

The arbitrary argument is a list of options.
The {\tt -delay} option makes {\tt Stages.totex} delay
filename markup until after the first documentation chunk; this
behavior makes the first documentation chunk a ``limbo''
chunk, which can usefully contain commands like \verb+\documentclass+.
The {\tt -noindex} option suppresses output relating to the index of
identifiers; it is used to implement {\tt noweave -x}.


\subsubsection{The sink \texttt{Stages.unmarkup}}

The sink \texttt{Stages.unmarkup} converts the contents of the
pipeline back to its ASCII input representation.
The arbitrary argument may be an open file; if it is, the results are
written to that file.
Otherwise, they are written to the standard Lua output file.

\subsection{Internal filters}
\subsubsection{The filter \texttt{Stages.elide}}
The filter \texttt{Stages.elide} removes code chunks from a stream.
It is most frequently used to remove ``code chunks'' that actually
contain documentation destined for a man page or user manual.
Its arbitrary argument is a list of strings, each of which is a ``glob
pattern.''
(In a glob pattern, \texttt{*} matches any string, and \texttt{?}
matches any single character.)
The filter removes definitions matching any of the glob patterns in
the arguments.
For example, this section of this manual
actually appears in a code chunk named
\emph{$\langle$hackers: subsections for internal filters$\rangle$}.
To prepare internal documentation for the stages, we use
\texttt{Stages.elide} with the argument \texttt{hackers:*}.

FIXME: Also undocumented \texttt{-eat} option.

\subsubsection{The filters \texttt{Stages.droptext} and \texttt{Stages.eattext}}
The filters \texttt{Stages.droptext} and \texttt{Stages.eattext}
remove text fragment chunks from a stream.
They are normally used early in the pipeline so as to remove whole lines.
For both filters, the arbitrary argument is either a single string or 
a list of strings.
Each string is a Lua string pattern, as documented on page~19 of the 
Lua~2.5$+$nw manual.
\texttt{Stages.droptext} simply removes text parts matching the pattern.
Because it preserves newlines, it can be used to ``blank out'' 
(replace with an empty line) code that is not wanted when tangling.
\texttt{Stages.eattext} replaces the matching text with
\verb+\eatline+.
It is useful for making lines disappear during weaving, without throwing off the 
line numbers in the \TeX\ file.
These filters exist only because they preserve newlines; otherwise, 
it would be simpler to use \texttt{grep~-v}.
\subsubsection{The filter \texttt{Stages.noidx}}

\texttt{Stages.noidx} computes all the index and cross-reference information
represented by the \nkw{INDEX} and \nkw{XREF} keys.
Its arbitrary argument is a list of strings denoting options.

The {\tt -delay}  option delays heading material until
after the first chunk, and brings trailing material before the last
chunk.
In particular, it causes
the list of chunks and the index of identifiers to be emitted before
the last chunk.

The {\tt -docanchor $n$} option sets the anchor for a code chunk.
\begin{enumerate}
\item
If a documentation chunk precedes the code chunk and is $n$ or more lines long,
 put the anchor $n$~lines from the end of that documentation chunk.
\item
If a documentation chunk precedes the code chunk and is fewer than $n$~lines long, 
put the anchor at the beginning of that documentation chunk.
\item
If no documentation chunk precedes the code chunk, 
put the anchor at the beginning of
the code chunk, just as if {\tt -docanchor} had not been used.
\end{enumerate}
This option is used to create anchors suitable for the HTML back end.
Its effect is to make hyperlinks jump to the documentation preceding a code chunk 
rather than to the code chunk itself.

The \texttt{-dbout \emph{file}} option causes the filter to emit, on
\emph{file}, information about the first and last definitions of
chunks in this file.
(What about uses?)

The \texttt{-dbin file} option notes that chunks may be linked in from
a preceding or succeeding file, as determined by
whether \texttt{-dbin} precedes or succeeds \texttt{-dbout}.



% FIXME: I think all of this prose has been covered by new stuff
% and it's safe to remove it.
%
%\citeN{ramsey:simplified} describes {\tt noweb} from a user's
%point of view, showing its simplicity and examples of its use.
%The {\tt noweb} tools are implemented as {\em pipelines}.
%Each pipeline begins with the {\tt noweb}
%source file.  Successive stages of the pipeline implement simple
%transformations of the source, until the desired result emerges from
%the end of the pipeline.  Figures
%\ref{fig:pipe-notangle}~and~\ref{fig:pipe-noweave} on
%page~\pageref{fig:pipe-notangle} show pipelines for
%{\tt notangle} and {\tt noweave}.  
%Pipelines are responsible for {\tt {\tt noweb}}'s
%extensibility, which enables its users to create new literate-programming
%features without having to write their own tools.
%This document explains how to change or extend {\tt noweb} by
%inserting or removing pipeline stages.
%Readers should be familiar with the {\tt {\tt noweb}} man pages, which
%describe the structure of {\tt {\tt noweb}} source files.
%
%{\tt markup}, which is the first stage in
%every pipeline, converts {\tt noweb} source to a representation easily
%manipulated by common Unix tools like {\tt sed} and {\tt awk}, simplifying
%the construction of later pipeline stages.  Middle stages add
%information to the representation.  {\tt notangle}'s final stage converts to
%code; {\tt noweave}'s final stages convert to TeX, LaTeX or HTML.
%A final stage for GNU {\tt troff -mm} is supplied
%in the distribution, but is not supported.
%Middle stages are called {\em filters}, by analogy with Unix filters.
%Final stages are called {\em back ends}, by analogy with
%compilers---they don't transform {\tt {\tt noweb}}'s intermediate
%representation; they emit something else.
%
%Version~2 used the Unix pipeline to implement the \texttt{noweb}
%pipeline, and it used Bourne shell scripts to specify pipelines.
%Version~3 is distinguished by having three implementations of
%pipelines:
%\begin{itemize}
%\item 
%The \emph{external pipeline} is equivalent to the Version~2 pipeline;
%it consists of separate programs that pass information using the ASCII
%pipeline representation documented in the first half of this Guide.
%Two implementations are included; one uses Unix pipes, and one uses
%temporary files.
%The temporary-file implementation may one day be generalized to
%support DOS, Windows, and Mac platforms.
%\item
%The \emph{internal pipeline} is an abstract datatype used to build
%pipelines within a \emph{single} program.
%The stages in such pipelines are simple functions that read from one
%buffer and write to another.
%Internal pipelines are much more efficient than external pipelines,
%because they avoid the overhead of spawning processes and converting
%to and from ASCII.
%The catch is that many \texttt{noweb} pipeline stages are not yet available in
%internal form.
%\item
%The \emph{mixed pipeline} creates a pipeline in which stages may be
%internal functions or external programs.
%The implementation transparently introduces conversions to and from
%ASCII so that the Right Thing happens.
%The mixed pipeline supports backwards compatibility for old external
%stages, plus it supports new external stages that users or third
%parties might write.
%\end{itemize}
%In Version~3, the shell scripts are replaced with scripts written in
%Lua$+$nw, a derivative of the embedded language Lua from PUC-Rio in
%Brazil \opencite{ierusalimschy:lua}.
%Lua is more portable than the Bourne shell; it can support internal stages
%as well as external ones; and Lua code can replace Icon code for  many
%purposes.
%An implementation of Lua is distributed with Version~3.
%Therefore, when the Icon code is replaced, it will be possible to
%distribute \texttt{noweb} as a single binary, plus scripts written in Lua.
%(The source code will, of course, continue to be available.)


\section{The ASCII pipeline representation}

%FIXME: This is the original prose.  It could profitably be chopped down,
%now that there's full explanation for the version 3 stuff.

In the external pipeline, stages pass information by reading and
writing ASCII files.
In such files,
every line begins with an at sign (\texttt{@}) and one of the
keywords shown in Table~\ref{table:keywords}.

The structural keywords 
represent the {\tt noweb} source syntax directly.
They must appear in particular orders that reflect the
structure of the source.

The tagging keywords can be inserted
essentially anywhere (within reason), and with some exceptions, they are not generated
by {\tt markup}.

The wrapper keywords
mark the beginning and end of file,
and they carry information about what formatters are supposed to do in
the way of leading and trailing boilerplate.
They are used by {\tt noweave} but not by {\tt notangle}, and they are
inserted directly by the {\tt noweave} shell script, not by {\tt markup}.

\begin{table}[t]
\noindent
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
% l2h macro ttitle 1 </td></tr><tr><td colspan=2 align=center><b>#1</b></td></tr><td>
\ttitle{Structural keywords}
\hline
@begin {\rm\it kind} $n$&Start a chunk\\
@end {\rm\it kind} $n$&End a chunk\\
@text {\rm\it string}&{\rm\it string} appeared in a chunk\\
@nl&A newline appeared in a chunk\\
@defn {\rm\it name}&The code chunk named {\rm\it name} is being defined\\
@use {\rm\it name}&A reference to code chunk named {\rm\it name}\\
@quote&Start of quoted code in a documentation chunk\\
@endquote&End of quoted code in a documentation chunk\\
\hline
\ttitle{Tagging keywords}
\hline
@file {\rm\it filename}&Name of the file from which the chunks came\\
@line $n$&Next text line came from source line $n$ in current file\\
@language {\rm\it language}&Programming language in which code is written\\
@index \ldots&Index information.\\
@xref \ldots&Cross-reference information.\\
\hline
\end{tabularx}\\
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Wrapper keywords}
\hline
@header {\rm\it formatter options}&
      First line, identifying formatter and options\\
@trailer {\rm\it formatter}&Last line, identifying formatter.\\
\hline
%\end{tabularx}\\
%\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Error keyword}
\hline
@fatal {\rm\it stagename} {\rm\it message}&
      A fatal error has occurred.\\
\hline
%\end{tabularx}\\
%\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Lying, cheating, stealing keyword}
\hline
@literal {\rm\it text}&
      Copy {\it text} to output.\\
\hline
\end{tabularx}

\caption{Keywords used in {\tt noweb}'s external pipeline representation}
\label{table:keywords}

\end{table}

\subsection{Structural keywords}

The structural  keywords represent the chunks in the {\tt noweb} source.
Each chunk is bracketed by a \kws{begin}{end} pair,
and the {\it kind} of chunk is either {\tt docs} or {\tt code}.
The \kw{begin} and \kw{end} are numbered; within a single file,
numbers must be monotonically increasing, but they need not be
consecutive.
Filters may change chunk numbers at will.

Depending on its kind, a chunk may contain {\em documentation} or {\em
code}. 
Documentation may contain text and newlines, represented by \kw{text}
and \kw{nl}.
It may also contain {\em quoted code} bracketed by
\kws{quote}{endquote}.
Every \kw{quote} must be terminated by an \kw{endquote} within the
same chunk.
Quoted code corresponds to the \verb+[[+\ldots \verb+]]+ construct in
the {\tt noweb} source.

Code, whether it appears in quoted code or in a code chunk,
may contain text and newlines, and also definitions and uses of
code chunks, marked with \kw{defn} and \kw{use}.
The first structural keyword in any code chunk must be \kw{defn}.
\kw{defn} may be preceded or followed by tagging keywords, but the
next structural keyword
must be \kw{nl}; 
together, the \kw{defn} and \kw{nl}
represent the initial \verb+<<chunk name>>=+
that starts the chunk (including the terminating newline).

A few facts follow from what's already stated above, but are probably
worth noting explicitly:
\begin{itemize}
\item 
Quoted code may not appear in code, nor may it appear in
\kw{defn} or \kw{use}.
{\tt noweave} back ends are encouraged to give \verb+[[+\ldots
\verb+]]+ special treatment when it appears in \verb+defn+ or
\verb+use+, so that the text contained therein is treated as if it
were quoted code.
\item
The text in chunks may be distributed among as many \kw{text}
keywords as desirable.  Any number of empty \kw{text} keywords are
permitted.  In particular, it is not realistic to expect that a single
line will be represented in a single \kw{text} (see the discussion of
{\tt finduses} on page~\pageref{finduses}).
\item
{\tt markup} will sometimes emit \kw{use} within
\kws{quote}{endquote}, for example from a source like \verb+[[<<chunk name>>]]+.
\item
No two chunks have the same number.
\item
Because later filters can change chunk numbers, no filter should
plant references to chunk numbers anywhere in the pipeline.
\end{itemize}

\subsection{Tagging keywords}

The structural keywords carry all the code and documentation that
appears in a {\tt noweb} source file.
The tagging keywords carry information about that code or
documentation.
The \kw{file} keyword carries the name of the source file from which the
following lines come.
The \kw{line} keyword give the line number of the next \kw{text} line
within the current file (as determined by the most recent \kw{file}
keyword). 
The only guarantee about where these appear is that {\tt markup}
introduces each new source file with a \kw{file} that appears between
chunks.
Most filters ignore \kw{file} and \kw{line}, but {\tt nt}
respects them, so that {\tt
notangle} can properly mark line numbers if some {\tt noweb}
filter starts moving lines around.

\subsubsection{Programming languages}

To support automatic indexing or prettyprinting, it's possible to
indicate the programming language in which a chunk is written.
The \kw{language} keyword may appear at most once between
each \kw{begin~code} and \kw{end code} pair.
Standard values of \kw{language} and their associated meanings are:
\begin{quote}
\begin{tabularx}{\linewidth}{@{}>{\ttfamily}lX@{}}
\texttt{awk}&awk\\
\texttt{c}&C\\
\texttt{c++}&C$++$\\
\texttt{caml}&CAML\\
\texttt{haskell}&Haskell\\
\texttt{html}&HTML\\
\texttt{icon}&Icon\\
\texttt{latex}&{\LaTeX} source\\
\texttt{lua}&Lua or Lua-2.5$+$nw\\
\texttt{lisp}&Lisp or Scheme\\
\texttt{make}&A Makefile\\
\texttt{m3}&Modula-3\\
\texttt{ocaml}&Objective CAML\\
\texttt{perl}&A perl script\\
\texttt{python}&Python\\
\texttt{sh}&A shell script\\
\texttt{sml}&Standard ML\\
\texttt{tex}&plain {\TeX}\\
\texttt{tcl}&tcl\\
\end{tabularx}
\end{quote}
If the \kw{language} keyword catches on, it may be useful to create an
automatic registry on the World-Wide Web.

I have made it impossible to place \kw{language} information directly
in a \texttt{noweb} source file.
My intent is that tools will identify the language of the root chunks
using any of several methods: conventional names of chunks, being told
on a command line, or identifying the language by looking at the
content of the chunks.
(Of these methods, the most practical is to name the root chunks after
the files to which they will be extracted, and to use the same naming
conventions as \texttt{make} to figure out what the contents are.)
A \texttt{noweb} filter will tag non-root chunks with the appropriate
\kw{language} by propagating information from uses to definitions.
(This filter does not currently exist. Contributions gratefully accepted.)




\subsubsection{Indexing and cross-reference concepts}


The index and cross-reference commands use \itlabel s, \ident s, and \tag s.
A \itlabel\ is a unique string generated to refer to some element of a
literate program.
They serve as labels or ``anchor points'' for back ends that are
capable of implementing their own cross-reference.
So, for example, the {\LaTeX} back end uses labels as arguments to \verb+\label+
and \verb+\ref+, and the HTML back end uses labels to name and refer
to anchors.
Labels never contain white space, which simplifies parsing.
The standard filters cross-reference at the chunk level, so that each
label refers to a particular code chunk, and all references to that
chunk use the same label.

An \ident\ refers to a source-language identifier.
{\tt noweb}'s concept of identifier is general; an identifier is
an arbitrary string.
It can even contain whitespace.
Identifiers are used as keys in the index; references to the same
string are assumed to denote the same identifier.

{\rm\it Tag\/}s are the strings used to identify components for
cross-reference in the final document.
For example, Classic {\tt WEB} uses consecutive ``section numbers'' to
refer to chunks.
{\tt noweb}, by default, uses ``sub-page references,'' e.g., ``24b''
for the second chunk appearing on page~24.
The HTML back end doesn't use any tags at all; instead, it
implements cross-referencing using the ``hot link'' mechanism.

The final step of cross-referencing involves generating tags and
associating a tag with each label.
All the existing back ends rely on a document formatter to do this
job, but that strategy might be worth changing.
Computing tags within a {\tt noweb} filter could be lots easier than
doing it in a formatter.
For example, a filter that computed sub-page numbers by grubbing in
{\tt .aux} files would be pretty easy to write, and it would eliminate
a lot of squirrelly {\LaTeX} code.


\subsubsection{Index information}

I've divided the index keywords into several groups.
There seems to be a plethora of keywords, but most of them are
straightforward representations of parts of a document produced by
{\tt noweave}.  Readers may want to have a sample of {\tt noweave}'s
output handy when studying this and the next section.

\begin{table}
\begin{center}
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Definitions, uses, and {\tt @ \%def}}
\hline
@index defn \ident&The current chunk contains a definition of \ident\\
@index localdefn \ident&The current chunk contains a definition of
               \ident, which is not to be visible outside this file\\
@index use \ident&The current chunk contains a use of \ident\\
@index nl \ident&A newline that is part of markup, not part of the chunk\\
\hline
\ttitle{Identifiers defined in a chunk}
\hline
@index begindefs&Start list of identifiers defined in this chunk\\
@index isused \itlabel&
        The identifier named in the following \ikw{defitem} is used in
        the chunk labelled by \itlabel\\
@index defitem \ident&
        \ident\ is defined in this chunk, and it is used in all the
        chunks named in the immediately preceding \ikw{isused}.\\
@index enddefs&End list of identifiers defined in this chunk\\
\hline
\ttitle{Identifiers used in a chunk}
\hline
@index beginuses&Start list of identifiers used in this chunk\\
@index isdefined \itlabel&
        The identifier named in the following \ikw{useitem} is defined in
        the chunk labelled by \itlabel\\
@index useitem \ident&
        \ident\ is used in this chunk, and it is defined in each of the
        chunks named in the immediately preceding \ikw{isdefined}.\\
@index enduses&End list of identifiers used in this chunk\\
\hline
\end{tabularx}\\
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{The index of identifiers}
\hline
@index beginindex&Start of the index of identifiers\\
@index entrybegin \itlabel\ \ident&
        Beginning of the entry for \ident, whose first definition is
        found at \itlabel\\
@index entryuse \itlabel&
        A use of the identifer named in the last \ikw{entrybegin}
        occurs at the chunk labelled with \itlabel.\\
@index entrydefn \itlabel&
        A definition of the identifer named in the last \ikw{entrybegin}
        occurs at the chunk labelled with \itlabel.\\
@index entryend&
        End of the entry started by the last \ikw{entrybegin}\\
@index endindex&End of the index of identifiers\\
\hline
\end{tabularx}
\end{center}
\caption{External Pipeline Indexing keywords}
\label{tab:index}
\vskip -5pt
\end{table}



\paragraph{Definitions, uses, and {\tt @ \%def}}

The \kw{index} keywords
\ikw{defn}, \ikw{use}, and \ikw{nl} are the only ones
that appear in {\tt markup}'s output, and thus
which can appear in any program.
They may appear only within the boundaries of a code chunk (\kws{begin
code}{end code}).
\ikw{defn} and \ikw{use} simply indicate that the current chunk
contains a definition or use of the identifier \ident\ which follows
the keyword.
The placement of \ikw{defn} need not bear a relationship to the
text of the definition, but \ikw{use} is normally followed by a
\kw{text} that contains the source-code text identified as the
use.%
\footnote{This property can't hold when one identifier is a prefix of
another; see the description of {\tt finduses} on page~\pageref{finduses}.}

Instances of \ikw{defn} normally come from one of two sources: either a
language-dependent recognizer of definitions, or a hand-written
\verb+@ %def+ line.
In the latter case, the line is terminated by a newline that is
neither part of a code chunk nor part of a documentation chunk.
To keep line numbers accurate, that newline can't just be abandoned,
but neither can it be represented by \kw{nl} in a documentation or
code chunk.
The solution is the \ikw{nl} keyword, which serves no purpose other
than to keep track of these newlines, so that back ends can produce
accurate line numbers.

Following a suggestion by Oren Ben-Kiki,
\ikw{localdefn} indicates a definition that is not to be visible
outside the current file.
It may be produced by a language-dependent recognizer or other filter.
Because I have questions about the need for \ikw{localdefn}, there is
officially no way to cause {\tt markup} to produce it.

\paragraph{Identifiers defined in a chunk}

The keywords from \ikw{begindefs} to \ikw{enddefs} are used to represent
a more complex data structure giving the list of identifiers defined
in a code chunk.
The constellation represents a list of identifiers; one \ikw{defitem}
appears for each identifier.
The group also tells in what other chunks each identifier is used;
those chunks are listed by \ikw{isused} keywords which appear just
before \ikw{defitem}.
The labels in these keywords appear in the order of the corresponding
code chunks, and there are no duplicates.

These keywords can appear anywhere inside a code chunk, but
filters are encouraged to keep these keywords together.
The standard
filters guarantee that only
\ikw{isused} and \ikw{defitem} appear between \ikw{begindefs} and
\ikw{enddefs}.
The standard filters put them at the end of the code chunk, which
simplifies translation by the {\LaTeX} back end, but that strategy
might change in the future.


It should go without saying, but the keywords in these and all similar
groups (including some \kw{xref} groups) must be properly structured.
That is to say:
\begin{enumerate}
\item
Every \ikw{begindefs} must have a matching \ikw{enddefs} within the
same code chunk.
\item
\ikw{isused} and \ikw{defitem} may appear only between matching
\ikw{begindefs} and \ikw{enddefs}.
\item
The damn things can't be nested.
\end{enumerate}


\paragraph{Identifiers used in a chunk}


The keywords from \ikw{beginuses} to \ikw{enduses} are the dual of
\ikw{begindef} to \ikw{enddef};
the structure lists the identifiers used in the current code chunk,
with cross-references to the definitions.
Similar interpretations and restrictions apply.
Note that an identifier can be defined in more than one chunk,
although we expect that to be an unusual event.
{\hfuzz=1.2pt\par}

\paragraph{The index of identifiers}

Keywords \ikw{beginindex} to \ikw{endindex} represent the
complete index of all the identifiers used in the document.
Each entry in the index is bracketed by \ikws{entrybegin}{entryend}.
An entry provides the name of the identifier, plus the labels of all the
chunks in which the identifier is defined or used.
The label of the first defining chunk
is given at the beginning of the entry so that back ends needn't
search for it.
{\hfuzz=4.9pt\par}

Filters are encouraged to keep these keywords together.
The standard filters put them almost at the very end of the {\tt
noweb} file, just before the optional \kw{trailer}.



\subsubsection{Cross-reference information}

\providecommand\anchor{{\rmfamily\textit{anchor}}}

% l2h substitution anchor <i>anchor</i>

The most basic function of the cross-referencing keywords is to
associate labels and pointers (cross-references) with elements of the
document, which is done with the \xkw{ref} and \xkw{label} keywords.
The other \kw{xref} keywords all express chunk cross-reference
information that is emitted directly by one or more back ends.

Chunk cross-reference introduces the idea of
an {\anchor}, which is a label that refers to an ``interesting point''
we identify with the
beginning of a code chunk.
The anchor is the place we expect to turn to when we want to know about a
code chunk;
its exact value and interpretation depend on the back end being used.
The standard {\LaTeX} back end uses the sub-page number of the
defining chunk as the anchor, but the standard HTML back end uses some
\kw{text} from the documentation chunk preceding the code chunk.


\begin{table}
\begin{center}
\begin{tabularx}{\textwidth}{|>{\tt}l>{\raggedright\arraybackslash}X|}
\ttitle{Basic cross-reference}
\hline
@xref label \itlabel&Associates \itlabel\ with tagged item.\\
@xref ref \itlabel&
        Cross-reference from tagged item to item associated with \itlabel.\\
\hline
\ttitle{Linking previous and next definitions of a code chunk}
\hline
@xref prevdef \itlabel&
   The \kw{defn} from the previous definition of this chunk is
   associated with \itlabel.\\
@xref nextdef \itlabel&
   The \kw{defn} from the next definition of this chunk is
   associated with \itlabel.\\
\hline
\ttitle{Continued definitions of the current chunk}
\hline
@xref begindefs&Start ``This definition is continued in \ldots''\\
@xref defitem \itlabel&Gives the label of a chunk in which the
definition of the current chunk is continued.\\
@xref enddefs&Ends the list of chunks where definition is continued.\\
\hline
\ttitle{Chunks where this code is used}
\hline
@xref beginuses&Start ``This code is used in \ldots''\\
@xref useitem \itlabel&Gives the label of a chunk in which this
    chunk is used.\\
@xref enduses&Ends the list of chunks in which this code is used.\\
@xref notused {\rm\it name}&
        Indicates that this chunk isn't used anywhere in this document.\\
\hline
\ttitle{The list of chunks}
\hline
@xref beginchunks&Start of the list of chunks\\
@xref chunkbegin \itlabel\ {\it name}&
        Beginning of the entry for chunk {\it name}, whose {\anchor}
        is found at \itlabel.\\
@xref chunkuse \itlabel&
        The chunk is used in the chunk labelled with \itlabel.\\
@xref chunkdefn \itlabel&
        The chunk is defined in the chunk labelled with \itlabel.\\
@xref chunkend&End of the entry started by the last \xkw{chunkbegin}\\
@xref endchunks&End of the list of chunks\\
\hline
\ttitle{Converting labels to tags}
\hline
@xref tag \itlabel\ \tag&Associates \itlabel\ with \tag.\\
\hline
\end{tabularx}
\end{center}
\vskip -4pt
\caption{External Pipeline Cross-referencing keywords}
\label{tab:xref}
\vskip -3pt
\end{table}



\paragraph{Basic cross-reference}

\xkw{label} and \xkw{ref} are named by analogy with the {\LaTeX}
\verb+\label+ and \verb+\ref+ commands.
\xkw{label} is used to associate a \itlabel\ with a succeeding item.
Items that can be so labelled include
\begin{quote}
\begin{tabularx}{\linewidth}{>{\tt}l>{\raggedright\arraybackslash}X}
@defn&Labels the code chunk that begins with this \rlap{\kw{defn}.}\\
                                                % cheating the line breaker
@use&Labels this particular use.\\
@index defn&Labels this definition of an identifier.\\
@index use&Labels this use of an identifier.\\
@text&Typically labels part of a documentation chunk.\\
@end docs&Typically labels an empty documentation chunk.\\
\end{tabularx}
\end{quote}
I haven't made up my mind whether this should be the complete set, but
these are the ones used by the standard filters.
Most back ends use the chunk as the basic unit of cross-reference, so
the labels of \kw{defn} are the ones that are most often used.
The HTML back end, however, does something a little different---it
uses labels that refer to documentation preceding a chunk, because
typical HTML browsers place the label%
\footnote{The HTML terminology calls a label an ``anchor.''}
at the top of the screen, and using the label of the \kw{defn} would
lose the documentation immediately preceding a chunk.
The labels used by this back end usually point to \kw{text}, but they
may point to \kw{end docs} when no text is available.


\xkw{ref} is used to associate a reference with a succeeding item.
Such items include
\begin{quote}
\begin{tabularx}{\linewidth}{l>{\raggedright\arraybackslash}X}
{\tt @defn}, {\tt @use}&Refers to the label used as an {\anchor} for this chunk.\\
\vtop{\hbox{\strut{\tt @index defn},}\hbox{\strut{\tt @index use}}}&
  Refers to the label used as an {\anchor} for the first
           chunk in which this identifier is defined.\\
\end{tabularx}
\end{quote}


\paragraph{Linking previous and next definitions of a code chunk}

\xkw{prevdef} and \xkw{nextdef} may appear anywhere in a code chunk,
and they give the labels of the preceding and succeeding definitions
of that code chunk, if any.
Standard filters currently put them at the beginning of the code
chunk, following the initial \kw{defn}, so the information can be used
on the \kw{defn} line,
\`a la \citeN{fraser:retargetable:book}.


\paragraph{Continued definitions of the current chunk}

The keywords ranging from \xkw{begindefs} to \xkw{enddefs} appear in the first
definition of each code chunk.
They provide the information needed by the ``This definition is
continued in \ldots'' message printed by the standard {\LaTeX} back
end.
They can appear anywhere in a code chunk, but standard filters put
them after all the \kw{text} and \kw{nl}s, so that back ends can just
print out text.


\paragraph{Chunks where this code is used}

The keywords from \xkw{beginuses} to \xkw{enduses} are the dual of
\xkw{begindefs} to \xkw{enddefs}; they show where the current chunk is
used.
As with \xkws{begindefs}{enddefs}, they appear only in the first
definition of any code chunk, and they come at the end.
Sometimes, as with root chunks, the code isn't used anywhere, in which
case \xkw{notused} appears instead of \xkws{beginuses}{enduses}.
The name of the current chunk appears as an argument to \xkw{notused}
because some back ends may want to print a special message for unused
chunks---they might be written to files, for example.

\paragraph{The list of chunks}

The list of chunks, which is defined by the keywords
\xkws{beginchunks}{endchunks}, is the
analog of the index of identifiers, but it lists all the code chunks
in the document, not all the identifiers.

Filters are encouraged to keep these keywords together.
The standard filters put them at the end of the {\tt
noweb} file, just before the index of identifiers.


\paragraph{Converting labels to tags}

None of the existing back ends actually computes tags; they all use
formatting engines to do the job.
The {\LaTeX} back end uses an elaborate macro package to compute
sub-page numbers, and the HTML back end arranges for ``hot links'' to
be used instead of textual tags.

Some people have argued that literate-programming tools shouldn't require
elaborate macro packages, that they should use the basic facilities
provided by a formatter.  Nuweb, for example, uses standard {\LaTeX}
commands only, but goes digging through {\tt .aux} files to find
labels and compute sub-page numbers.
Doing this kind of computation in a real programming language is much
easier than doing it with {\TeX} macros, and I expect that one day
{\tt noweb} will have a tag-computing filter, the results of
which will be expressed using the \xkw{tag} keyword.

The rules governing \xkw{tag} are that it can appear anywhere.
None of the standard filters or back ends does anything with it.



\subsection{Wrapper keywords}

The wrapper keywords, \kw{header} and \kw{trailer}, are anomalous in
that they're not generated by {\tt markup} or by any of the
standard filters; instead they're inserted by the {\tt noweave} shell
script at the very beginning and end of file.
The standard {\TeX}, {\LaTeX}, and HTML back ends use them to provide
preamble and postamble markup, i.e., boilerplate that usually has to
surround a document.
They're not required (sometimes you don't want that boilerplate), but
when they appear they must be the very first and last lines in the
file, and the formatter names must match.

\subsection{Error keyword}

The error keyword \kw{fatal} signifies that a fatal error has
occurred.
The pipeline stage originating such an error gives its own name and a
message, and it also writes a message to standard error.
Filters seeing \kw{fatal} must copy it to their output and terminate
themselves with error status.
Back ends seeing \kw{fatal} must terminate themselves with error
status. (They should not write anything to standard error since that
will have been done.)

Using \kw{fatal} enables shell scripts to detect
that something has gone wrong even if the only exit status they have
access to is the
exit status of the last stage in a pipeline.

\subsection{Lying, cheating, stealing keyword}

The \kw{literal} keyword is used to hack output directly into \texttt{noweave}
back ends, like \texttt{totex} and \texttt{tohtml}.
These back ends simply copy the text to their output.
Tangling back ends ignore \kw{literal}.
The \kw{literal} keyword is used by Master Hackers who are too lazy to
write new back ends.
Its use is deprecated.
It should not exist.





\section{Standard external filters}

All the standard filters, unless otherwise noted, read the {\tt noweb}
keyword format on standard input and write it on standard output.
Some filters may also use auxiliary files.

\subsection{\tt markup}

Strictly speaking, {\tt markup} is a front end, not a filter, but I
discuss it along with filters because it generates the output that is
massaged by all the filters.
{\tt markup}'s output represents a sequence of files.
Each file is represented by a ``{\tt @file~{\rm\it filename}}'' line,
followed by a sequence of chunks.
{\tt markup} numbers chunks consecutively, starting at~0.
It also recognizes and undoes the escape sequence for double brackets,
e.g.~converting ``{\tt @{}<<}'' to ``{\tt <<}''.
The only tagging keywords found in its output are \ikw{defn} or
\ikw{nl}; despite what's written about it, \ikw{use} never appears.

\subsection{\tt autodefs.*}

I've written half a dozen language-dependent filters that use simple
heuristics (``fuzzy parsing'' if you prefer) to try to identify
interesting definitions of identifiers.
Many of these doubtless rely on my own idiosyncratic coding styles,
but all of them provide good value for little effort.
None of them does anything more complicated than scan individual
\kw{text} lines in code chunks, spitting out \ikw{defn}
and \ikw{localdefn} lines after
the \kw{text} line whenever it thinks it's found something.
All the filters are written in Icon and use a central core defined in
\verb+icon/defns.nw+. 
The C filter is the most complicated; it actually tries to understand
parts of the C grammar for declarations.
None of these filters has any command-line options.

\subsection{\tt finduses}

\label{finduses}

Using code contributed by Preston Briggs, this filter makes two passes
over its input.
The first pass reads in all the \ikw{defn} and \ikw{localdefn} lines and builds an
Aho-Corasick recognizer%
\footnote{A.V.~Aho and M.~Corasick.
Efficient string matching: An aid to bibliographic search. Comm. ACM, 18(6):333--340, 1975.
This is the algorithm used for the Unix \texttt{fgrep} program.}
for the identifiers named therein.
The second pass copies the input, searching for these identifiers in
each \kw{text} line that is code.
When it finds an identifier, {\tt finduses} breaks the \kw{text} line
into pieces, inserting \ikw{use} immediately before the \kw{text}
piece that contains the identifier just found.%
\footnote{The behavior described would duplicate \kw{text} pieces
whenever one identifier was a prefix of another.
This event is rare, and probably undesirable, but it can happen if,
for example, the C$++$ names {\tt MyClass} and {\tt MyClass::Function}
are both considered identifiers.
In this case, whatever identifier is found first is emitted first, and
only the unemitted pieces of longer identifiers are emitted.}
{\tt finduses} assumes that previous filters will not have broken
\kw{text} lines in the middle of identifiers.


The \verb+-noquote+ command-line option prevents {\tt finduses} from
searching for uses in quoted code.
If {\tt finduses} is given arguments, it takes those arguments to be
file names, and it reads lists of identifiers (one per line) from the
files so named, rather than from its input.
This technique enables {\tt finduses} to make a single pass over its
input; {\tt noweave} uses it to implement the {\tt -indexfrom} option.

{\tt finduses} shouldn't be run before filters which, like the {\tt
autodefs} filters, expect one line to be represented in a single
\kw{text}.
Filters (or back ends) that have to be run late, like 
prettyprinters, should be prepared to deal with lines broken into
pieces and with \kw{index} and \kw{xref} tags intercalated.

\subsection{\tt noidx}

{\tt noidx} computes all the index and cross-reference information
represented by the \kw{index} and \kw{xref} keywords.

The {\tt -delay} command-line option delays heading material until
after the first chunk, and brings trailing material before the last
chunk.
In particular, it causes
the list of chunks and the index of identifiers to be emitted before
the last chunk.

The {\tt -docanchor $n$} option sets the anchor for a code chunk to be
either:
\begin{enumerate}
\item
If a documentation chunk precedes the code chunk and is $n$ or more lines long, $n$
lines from the end of that documentation chunk.
\item
If a documentation chunk precedes the code chunk and is fewer than $n$
lines long, at the beginning of that documentation chunk.
\item
If no documentation chunk precedes the code chunk, at the beginning of
the code chunk, just as if {\tt -docanchor} had not been used.
\end{enumerate}
This option is used to create anchors suitable for the HTML back end.


\section{Standard external back ends}

\subsection{\tt nt}

The {\tt nt} back end implements {\tt notangle}.
It extracts the program defined by a single code chunk (expanding all
uses to form their definitions) and writes that program on standard
output.
Its command-line options are:
\begin{quote}
\begin{tabularx}{\linewidth}{lX}
\tt -t&Turn off expansion of tabs.\\
\tt -t$n$&Expand tabs on $n$-column boundaries.\\
\tt -R{\rmfamily\textit{name}}&Expand the code chunk named \textit{name}.\\
\tt -L{\rmfamily\textit{format}}&Use \textit{format} as the format string
        to emit line-number information.
\end{tabularx}
\end{quote}
See the man page for {\tt notangle} for details on the operation of
{\tt nt}.


\subsection{\tt mnt}

{\tt mnt} (for Multiple NoTangle) 
is a back end that can extract several code chunks from a
single document in a single pass.  It is used to make the {\tt noweb}
shell script more efficient.
In addition to the {\tt -t} and {\tt -L} options recognized by {\tt
nt}, it recognizes {\tt -all} as an instruction to extract and write
to files all of the code chunks that conform to the rules set out in
the {\tt noweb} man page.
It also accepts arguments, as well as options; each argument is taken
to be the name of a code chunk that should be emitted to the file of
the same name.
Unlike {\tt nt}, {\tt mnt} has the function of {\tt cpif} built
in---it writes to a temporary file, then overwrites an existing file
only if the temporary file is different.



\subsection{\tt tohtml}

This back end emits HTML.
It uses the formatter {\tt html} with \kw{header} and \kw{trailer} to
emit suitable HTML boilerplate.
For other formatters (like {\tt none}) it emits no header or trailer.
Its command-line options are:
\begin{quote}
\begin{tabularx}{\linewidth}{lX}
\tt -delay&Accepted, for compatibility with other back ends, but ignored.\\
\tt -localindex&Produces local identifier cross-reference after each code chunk.\\
\tt -raw&Wraps text generated for code chunks in a {\LaTeX} {\tt rawhtml}
environment, making the whole document suitable for processing with
{\tt latex2html}.\\
\end{tabularx}
\end{quote}


\subsection{\tt totex}

{\tt totex} implements both the plain {\TeX} and {\LaTeX} back ends,
using \kw{header tex} and \kw{header latex} to distinguish them.
When using a {\LaTeX} header, {\tt totex} places the optional text
following the header inside a \verb+\noweboptions+ command.

On the command line, the {\tt -delay} option makes {\tt totex} delay
filename markup until after the first documentation chunk; this
behavior makes the first documentation chunk a ``limbo''
chunk, which can usefully contain commands like \verb+\documentclass+.
The {\tt -noindex} option suppresses output relating to the index of
identifiers; it is used to implement {\tt noweave -x}.
{\hfuzz=1.2pt\par}

\subsection{\tt toroff}

The {\emph unsupported} {\tt toroff} back end generates {\tt troff} output,
suitable for use with both the {\tt -mm} and {\tt -ms} macro packages.
It uses its own set of additional macros. This macro package is designed
for use with GNU {\tt troff}; it has not been tested with standard Unix
{\tt troff}.

The {\tt noroff} program handles invoking {\tt noweb} correctly, as well
as actually running {\tt troff}.  The {\tt -delay} command line
option allows the initial document chunk to be processed
before invoking the ``begin document'' macro,
so that necessary initialization may
be performed.  The only commands which are useful in such an initial chunk
are bare {\tt troff} commands.

\subsection{\tt unmarkup}

{\tt unmarkup} attempts to  be the inverse of {\tt markup}---a document
already in the pipeline is converted back to {\tt noweb} source form.
This back end is useful primarily for trying to convert other literate
programs to {\tt noweb} form.
It might also be used to capture and edit the output of an automatic
definition recognizer.

\section{Standard commands}

FIXME: This section needs reevaluating.


\begin{figure}[t]
\noindent
\begin{tabbing}
XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\={}\kill
\>\+{\tt markup}: Convert to pipeline representation\+\\
{\tt nt:} Extract desired chunk to standard output
\end{tabbing}
\caption{Stages in pipeline for {\tt notangle}}
\label{fig:pipe-notangle-external}

\noindent
\begin{tabbing}
XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\=XXl\={}\kill
\>\+{\tt markup}: Convert to pipeline representation\+\\
{\tt autodefs.c}: Find definitions in C code\+\\
{\tt finduses -noquote}: Find uses of defined identifiers\+\\
{\tt noidx}: Add index and cross-reference information\+\\
{\tt totex}: Convert to {\LaTeX}
\end{tabbing}
\caption{Stages in pipeline for {\tt noweave -index -autodefs c}}
\label{fig:pipe-noweave-external}
\end{figure}


The Version~2 commands are all written as Bourne shell scripts
\opencite{kernighan:unix}. 
They assemble Unix pipelines using {\tt markup} and the filters and
back ends described above.  They are documented in man pages, and
there is no sense in repeating that material here.
I do show two sample pipelines in
Figures \ref{fig:pipe-notangle-external}~and~\ref{fig:pipe-noweave-external}.
The source code is available in the {\tt shell} directory for those
who want to explore further.

The Version~3 commands are written as Lua scripts.
Version~3 uses a new style; the command:
\begin{quote}
\texttt{no weave} \emph{arguments}
\end{quote}
searches for a Lua script named \texttt{weave}, then executes that
script, passing the \emph{arguments} in a Lua variable named
\texttt{argv}.
Look at \texttt{lua/weave.nw} and \texttt{lua/tangle.nw} for examples.
({\tt weave} is way too complicated.)




\section{Examples}

I don't give examples of the pipeline
representation; it's best just to
play with the existing filters.
In particular, 
\begin{quote}
{\tt noweave -v} {\it options} {\it inputs} {\tt >/dev/null}
\end{quote}
prints (on standard error) the pipeline used by {\tt noweave}
to implement any set of {\it options}.
In this section, I give examples of a few nonstandard external filters I've
thrown together for one purpose or another.

{\hfuzz=6.8pt
This one-line {\tt sed} command makes {\tt noweb} treat two chunk names as
identical if they differ only in their representation of whitespace:
\begin{verbatim}
 sed -e '/^@use /s/[ \t][ \t]*/ /g' -e '/^@defn /s/[ \t][ \t]*/ /g'
\end{verbatim}
\par}

This little filter, a Bourne shell script
written in {\tt awk} \opencite{aho:awk},
makes the definition of an empty chunk (\verb+<<>>=+)
stand for a continuation of the previous chunk definition.
\begin{verbatim}
awk 'BEGIN { lastdefn = "@defn " }
/^@defn $/ { print lastdefn; next }
/^@defn /  { lastdefn = $0 }
{ print }' "$@"
\end{verbatim}



To share programs with colleagues who don't enjoy literate
programming, I use a filter, shown in Figure~\ref{fig:nountangle}, that
places each line of documentation in a comment and moves it to
the succeeding code chunk.
With this filter, \verb+notangle+
transforms a literate 
program into a traditional commented program, without loss of
information and with only a modest penalty in readability.


\begin{figure}[p]
\begin{verbatim}
awk 'BEGIN { line = 0; capture = 0
             format = sprintf("'"$format"'",'"$width"')
           }
function comment(s) {
    '"$subst"'
    return sprintf(format,s)
}

function grab(s) {
  if (capture==0) print
  else holding[line] = holding[line] s
}
   
/^@end doc/ { capture = 0; holding[++line] = "" ; next }
/^@begin doc/ { capture = 1; next }

/^@text /     { grab(substr($0,7)); next}
/^@quote$/    { grab("[[") ; next}
/^@endquote$/ { grab("]]") ; next}

/^@nl$/ { if (capture !=0 ) {
            holding[++line] = ""
          } else if (defn_pending != 0) {
            print "@nl"
            for (i=0; i<=line && holding[i] ~ /^ *$/; i++) i=i
            for (; i<=line; i++) 
              printf "@text %s\n@nl\n", comment(holding[i])
            line = 0; holding[0] = ""
            defn_pending = 0
          } else print
          next  
        }

/^@defn / { holding[line] = holding[line] "<"substr($0,7)">="
            print ; defn_pending = 1 ; next }
{ print }'
\end{verbatim}
\caption{{\tt awk} command used to transform documentation to comments}

\smallskip
\noindent
\verb+$subst+, \verb+$format+, and \verb+$width+ are shell variables used
to adapt the script for different languages.
Executing \verb+$subst+ eliminates comment-end markers (if any) from
the documentation, and the initial \verb+sprintf+ that creates the
{\tt awk}
variable \verb+format+ gives the format used to print a line of
documentation as a comment.

\label{fig:nountangle}
\end{figure}

\afterpage{\clearpage} % force figures out


As a demonstration, and to help convert Nuweb programs to {\tt
noweb}, I wrote a
a 55-line Icon program that makes it possible to abbreviate chunk names
using a trailing ellipsis, as in {\tt WEB}; it appears in the {\tt
noweb} 2 distribution as
\verb+icon/disambiguate.nw+. 

Kostas Oikonomou of AT\&T Bell Labs and Conrado Martinez-Parra of 
the Univ.\ Politecnica de Catalunya in Barcelona have written filters
that add prettyprinting to {\tt noweb}.
Oikonomou's filters prettyprint Icon and Object-Oriented Turing;
Martinez-Parra's filter prettyprints a variant of Dijkstra's language
of guarded commands.
These filters are in the {\tt noweb} distribution in the \verb+contrib+ directory.

It's also possible to do useful or amusing things by writing new back
ends.
Figure~\ref{fig:nocount} shows an {\tt awk} script that gives a count of the
number of lines of code and of documentation in a group of {\tt noweb}
files.

\begin{figure}[!b]
\begin{verbatim} 
BEGIN { bogus = "this is total bogosity"
        codecount[bogus] = -1; docscount[bogus] = -1 
      }
/^@file / { thisfile = $2 ; files[thisfile] = 0 }
/^@begin code/ { code = 1 }
/^@begin docs/ { code = 0 }
/^@nl/ {
  if (code == 0)
    docscount[thisfile]++
  else
    codecount[thisfile]++
}
END { 
  printf " Code   Docs   Both  File\n"
  for (file in files) {
    printf "%5d  %5d  %5d  %s\n", 
        codecount[file], docscount[file],
        codecount[file]+docscount[file], file
    totalcode += codecount[file]
    totaldocs += docscount[file]
  } 
  printf "%5d  %5d  %5d  %s\n", 
      totalcode, totaldocs, totalcode+totaldocs, "Total"
}
\end{verbatim}
\caption{Back end for counting lines of code and documentation}
\label{fig:nocount}

\smallskip

\noindent
The \verb+BEGIN+ code forces \verb+codecount+ and
\verb+docscount+ to be associative arrays; without it, bugs in some
versions of Awk would cause the increment
operations associated with \verb+@nl+ to fail.
\end{figure}

\section{Acknowledgements}

Garret Prestwood did much of the preliminary work leading to
Version~3, in particular, the pipeline implementations.


\bibliographystyle{nchicago}
\bibliography{web,ramsey,cs}

\end{document}
